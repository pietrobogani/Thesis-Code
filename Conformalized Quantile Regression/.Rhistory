mean_squared_diff <- mean((zST_ecdf-rvec)^2) # h = 4: 0.02338473 || h = 1: 0.005109203
zSTGDPonly_ecdf <- PITtest_env$PITtest(PitSTGDPonly_OOS, rvec)
CnSS_model_GDPonly <- 1 - mean((zSTGDPonly_ecdf - seq(0, 1, length.out = length(zSTGDPonly_ecdf)))^2) / var(zSTGDPonly_ecdf) # h = 4: 0.872747225 || h = 1: 0.95697 is the value. 1 is perfect calibrated
mean_squared_diff_GDPonly <- mean((zSTGDPonly_ecdf-rvec)^2) # h = 4: 0.009966178 || h = 1: 0.003439986
if (h == 1) {
# Use asymptotic 5% critical value from Rossi and Sekhposyan (2017): 1.34
kappa <- 1.34
kappaGDPonly <- 1.34
} else if (h == 4) {
# Compute bootstrapped 5% critical values
PITs <- cbind(PitST_OOS, PitSTGDPonly_OOS)
PITs <- PITs[(jtFirstOOS + h):nrow(PITs), , drop = FALSE]
#testcritvalues <- matrix(NA, nrow = 1, ncol = 2, dimnames = list(NULL, c('GDP and NFCI', 'GDP only')))
testcritvalues <- array(NA, dim = c(2, 3, 2))
for (i in 1:2) {
testcritvalues[,, i] <- round(rstestboot_env$rstestboot(PITs[, i])$critvalues[2] * 100) / 100
}
kappa <- testcritvalues[1, 2, 1] #different from Matlab due to seed in CVfinalbootstrapInoue
kappaGDPonly <- testcritvalues[1, 2, 2] #different from Matlab due to seed in CVfinalbootstrapInoue
}
# Plot PIT for full quantile regression vs. quantile regression with GDP only
plot(rvec, zST_ecdf, type = 'l', col = 'blue', xlab = '??', ylab = 'Empirical CDF')
lines(rvec, zSTGDPonly_ecdf, type = 'l', col = 'red')
P <- sum(!is.na(PitST_OOS)) #correct for both h = 1 and h = 4
lines(rvec, rvec - (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec, rvec + (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec,rvec , col = 'black',lty=2)
legend('bottomright', legend = c('GDP and NFCI', 'GDP only', 'Theoretical and 5% Critical Values'), cex = 0.5,fill = c('blue', 'red', 'black'))
# Install and load necessary packages
library(quantreg)
library(lubridate)
library(pracma)
library(readxl)
library(sn)
library(quantreg)
# Set forecast horizon (run script separately for h = 1 and h = 4)
h <- 1
filename <- paste("ResOOS_H1", h, ".RData", sep="")
cat(paste("Loading results from file", filename, "\n"))
load(filename)
PITtest_env <- new.env()
source("PITtest.r",local = PITtest_env)
rstestboot_env <- new.env()
source("rstestboot.r",local = rstestboot_env)
# (a)/(b) Quantiles
par(mar = c(3, 3, 2, 1))  # Adjust the values as needed (bottom, left, top, right)
plot(Time, YQ_OOS[, jq05], type = 'l', col = 'blue', xlab = 'Time', ylab = 'Quantiles', xlim = range(Time),ylim = c(-20,20))
lines(Time, YQ_OOS[, jq50], type = 'l', col = 'blue', lty = 2)
lines(Time, YQ_OOS[,jq95], type = 'l', col = 'blue', lty = 3)
lines(Time, YQ_IS[, jq05], type = 'l', col = 'black', lty = 1)
lines(Time, YQ_IS[, jq50], type = 'l', col = 'black', lty = 2)
lines(Time, YQ_IS[, jq95], type = 'l', col = 'black', lty = 3)
si calcola con i dati fino alla fine 1992. Se ho fatto un buon lavoro il 5% dei valori futuri cadranno dentro.
#Per fare il grafico PIT, credo di dover ripetere sta roba per molti intervalli di confidenza, non solo il 5%. Per ognuno di essi calcolo che % di punti sta
#dentro il mio intervallo (cioÃ¨ creo la empirical CDF). Infine plotto avendo sull'asse x l'ampiezza teorica dell'intervallo di confidenza.
#dovrebbe essere giusto
# Install and load necessary packages
library(quantreg)
library(lubridate)
library(pracma)
library(readxl)
library(sn)
# Clear workspace
rm(list = ls())
# Set forecast horizon (run script separately for h = 1 and h = 4)
h <- 1
loadsavedresults = FALSE; # If I run code already and results are stored in ResOOS_H
# Graphics settings - R's graphics system differs from MATLAB's, so some modifications are necessary
par(mfrow = c(1, 1))  # Reset plot window to single pane
# Load data
file_path <- "DataVulnerabilityAppendix.xls"
# Read the file
data <- read_excel(file_path)
data<-data[,1:3]
# Filter data for 1973Q1-2015Q4
colnames(data)[1] <- "Time"
data$Time <- as.Date(data$Time)
# Subset the data
data <- data[data$Time >= as.Date("1973-01-01") & data$Time <= as.Date("2015-10-01"), ]
X <- data[,2:3]
Time <- data$Time
# Set forecast settings
QQ <- seq(0.05, 0.95, by = 0.05)
deltaYY <- 0.1
YY <- seq(-20, 20, by = deltaYY)
jtFirstOOS <- which(year(data$Time) == 1993 & month(data$Time) == 1)
indices <- which(QQ %in% c(0.05, 0.25, 0.5, 0.75, 0.95))
jq05 <- indices[1]
jq25 <- indices[2]
jq50 <- indices[3]
jq75 <- 15 #piccolo cheatcode per farlo venire
jq95 <- 19 #piccolo cheatcode per farlo venire
# Construct average growth rates
y <- X$A191RL1Q225SBEA
Yh <- matrix(0, nrow=length(y), ncol=4)
Yh <- filter(y, rep(1/h, h), sides=1)
if (h>1){
Yh[1:(h-1)] <- NA
}
#Construct matrices of regressors
Z <- cbind(1, X[,2], y)
ZGDPonly <- cbind(1, y)
Z <-as.matrix(Z)
# Get length of Time and QQ/YY
len_time <- length(data$Time)
len_qq <- length(QQ)
len_yy <- length(YY)
# Initialize matrices to store forecasts
# Raw quantiles
YQ_NaNs <- matrix(NA, len_time, len_qq)
#YQ_low_adj_IS <- YQ_NaNs #IN the original script I've just YQ_IS, because I do a point estimate of the quantile. Here I do a confidence interval estimation and I need 2
#YQ_high_adj_IS <- YQ_NaNs
YQ_low_IS <- YQ_NaNs
YQ_high_IS <- YQ_NaNs
YQ_IS <- YQ_NaNs
#YQ_low_adj_OOS <- YQ_NaNs
#YQ_high_adj_OOS <- YQ_NaNs
YQ_low_OOS <- YQ_NaNs
YQ_high_OOS <- YQ_NaNs
YQ_OOS <- YQ_NaNs
YQ_lowGDPonly_IS <- YQ_NaNs
YQ_highGDPonly_IS <- YQ_NaNs
YQGDPonly_IS <- YQ_NaNs
YQGDPonly_OOS <- YQ_NaNs
YQunc_IS <- YQ_NaNs
YQunc_OOS <- YQ_NaNs
YQunclow_IS <- YQ_NaNs
YQunchigh_IS <- YQ_NaNs
YQGDPonly_high_OOS <- YQ_NaNs
YQGDPonly_low_OOS <- YQ_NaNs
YQunclow_OOS <- YQ_NaNs
YQunchigh_OOS <- YQ_NaNs
YQunchigh_OOS <- YQ_NaNs
YQunclow_OOS <- YQ_NaNs
# PDFs (evaluated over grid)
P_NaNs <- matrix(NA, len_time, len_yy)
PST_IS <- P_NaNs
PST_OOS <- P_NaNs
PSTGDPonly_IS <- P_NaNs
PSTGDPonly_OOS <- P_NaNs
PSTunc_IS <- P_NaNs
PSTunc_OOS <- P_NaNs
# Smoothed quantiles
Q_NaNs <- matrix(NA, len_time, len_qq)
QST_IS <- Q_NaNs
QST_OOS <- Q_NaNs
QSTGDPonly_IS <- Q_NaNs
QSTGDPonly_OOS <- Q_NaNs
QSTunc_IS <- Q_NaNs
QSTunc_OOS <- Q_NaNs
# CDFs (evaluated over grid)
C_NaNs <- matrix(NA, len_time, len_yy)
CST_IS <- C_NaNs
CST_OOS <- C_NaNs
CSTGDPonly_IS <- C_NaNs
CSTGDPonly_OOS <- C_NaNs
CSTunc_IS <- C_NaNs
CSTunc_OOS <- C_NaNs
# Skewed t-distribution parameters
STpar_NaNs <- matrix(NA, len_time, 4)
STpar_IS <- STpar_NaNs
STpar_OOS <- STpar_NaNs
STparGDPonly_IS <- STpar_NaNs
STparGDPonly_OOS <- STpar_NaNs
STparunc_IS <- STpar_NaNs
STparunc_OOS <- STpar_NaNs
# Predictive scores
Score_NaNs <- rep(NA, len_time)
ScoreST_IS <- Score_NaNs
ScoreST_OOS <- Score_NaNs
ScoreSTGDPonly_IS <- Score_NaNs
ScoreSTGDPonly_OOS <- Score_NaNs
ScoreSTunc_IS <- Score_NaNs
ScoreSTunc_OOS <- Score_NaNs
# Probability integral transforms
Pit_NaNs <- rep(NA, len_time)
PitST_IS <- Pit_NaNs
PitST_OOS <- Pit_NaNs
PitSTGDPonly_IS <- Pit_NaNs
PitSTGDPonly_OOS <- Pit_NaNs
PitSTunc_IS <- Pit_NaNs
PitSTunc_OOS <- Pit_NaNs
# Left entropy
Entropy_NaNs <- rep(NA, len_time)
LeftEntropy_IS <- Entropy_NaNs
LeftEntropy_OOS <- Entropy_NaNs
#Split creating I1 and I2
full_length <- length(Yh[(h + 1):length(Yh)])
#permuted_indices <- c(1,2,3,4,sample((h+1):length(Yh))) #if you want to permute
permuted_indices <- c(1:length(Yh)) # if you don't want to permute
Yh_perm <- Yh[permuted_indices]
Z_perm <- Z[permuted_indices,]
ZGDPonly_perm <- ZGDPonly[permuted_indices,]
Yh1 <- Yh_perm[(h+1):(h+full_length/2)]
Yh2 <- Yh_perm[(h+1+full_length/2):length(Yh)]
Z1 <- Z_perm[1:(full_length/2),]
Z2 <- Z_perm[(full_length/2+1):(length(Yh) - h),]
ZGDPonly1 <- ZGDPonly_perm[1:(full_length/2),]
ZGDPonly2 <- ZGDPonly_perm[(full_length/2+1):(length(Yh) - h),]
filename <- paste("ResOOS_H1", h, ".RData", sep="")
cat(paste("Loading results from file", filename, "\n"))
load(filename)
PITtest_env <- new.env()
source("PITtest.r",local = PITtest_env)
rstestboot_env <- new.env()
source("rstestboot.r",local = rstestboot_env)
# (a)/(b) Quantiles
par(mar = c(3, 3, 2, 1))  # Adjust the values as needed (bottom, left, top, right)
plot(Time, YQ_OOS[, jq05], type = 'l', col = 'blue', xlab = 'Time', ylab = 'Quantiles', xlim = range(Time),ylim = c(-20,20))
lines(Time, YQ_OOS[, jq50], type = 'l', col = 'blue', lty = 2)
lines(Time, YQ_OOS[,jq95], type = 'l', col = 'blue', lty = 3)
lines(Time, YQ_IS[, jq05], type = 'l', col = 'black', lty = 1)
lines(Time, YQ_IS[, jq50], type = 'l', col = 'black', lty = 2)
lines(Time, YQ_IS[, jq95], type = 'l', col = 'black', lty = 3)
# (c)/(d) Downside Entropy
plot(Time, LeftEntropy_OOS, type = 'l', col = 'blue',
xlab = 'Time', ylab = 'Entropy', xlim = range(Time))
lines(Time, LeftEntropy_IS, type = 'l', col = 'black', lty = 2)
# Figure 11. Out-of-sample Accuracy.
# (a)/(b) Predictive scores
plot(Time, ScoreST_OOS, type = 'l', col = 'blue', xlab = 'Time', ylab = 'Scores')
lines(Time, ScoreSTGDPonly_OOS, type = 'l', col = 'black', lty = 2)
legend('topleft', legend = c('GDP and NFCI', 'GDP only'))
setwd("C:/Users/Pietro/Desktop/Pietro/Politecnico/Tesi/Thesis-Code/R version of Adrian's code")
filename <- paste("ResOOS_H", h, ".RData", sep="")
cat(paste("Loading results from file", filename, "\n"))
load(filename)
#------------------------------
PITtest_env <- new.env()
source("PITtest.r",local = PITtest_env)
rstestboot_env <- new.env()
source("rstestboot.r",local = rstestboot_env)
# Figure 10. Out-of-sample Predictions.             #CORRETTO!!!!!!!!
# (a)/(b) Quantiles
par(mar = c(3, 3, 2, 1))  # Adjust the values as needed (bottom, left, top, right)
plot(Time, YQ_OOS[, jq05], type = 'l', col = 'blue', xlab = 'Time', ylab = 'Quantiles', xlim = range(Time),ylim = c(-20,20))
lines(Time, YQ_OOS[, jq50], type = 'l', col = 'blue', lty = 2)
lines(Time, YQ_OOS[,jq95], type = 'l', col = 'blue', lty = 3)
lines(Time, YQ_IS[, jq05], type = 'l', col = 'black', lty = 1)
lines(Time, YQ_IS[, jq50], type = 'l', col = 'black', lty = 2)
lines(Time, YQ_IS[, jq95], type = 'l', col = 'black', lty = 3)
# (c)/(d) Downside Entropy        #CORRETTO!!!!!!!!!
plot(Time, LeftEntropy_OOS, type = 'l', col = 'blue',
xlab = 'Time', ylab = 'Entropy', xlim = range(Time))
lines(Time, LeftEntropy_IS, type = 'l', col = 'black', lty = 2)
# Figure 11. Out-of-sample Accuracy.   #CORRETTO!!!!!
# (a)/(b) Predictive scores
plot(Time, ScoreST_OOS1, type = 'l', col = 'blue', xlab = 'Time', ylab = 'Scores')
lines(Time, ScoreSTGDPonly_OOS1, type = 'l', col = 'black', lty = 2)
legend('topleft', legend = c('GDP and NFCI', 'GDP only'))
# (c)/(d): PITs
# The code below was modified from files provided by Barbara Rossi and
# Tatevik Sekhposyan implementing the specification tests for predictive
# densities described in Rossi and Sekhposyan (2017).
rvec <- seq(0, 1, by = 0.001)
zST_ecdf1 <- PITtest_env$PITtest(PitST_OOS, rvec)
CnSS_model <- 1 - mean((zST_ecdf1 - seq(0, 1, length.out = length(zST_ecdf1)))^2) / var(zST_ecdf1) # h = 4: 0.55658995 || h = 1: 0.7951442  is the value. 1 is perfect calibrated
mean_squared_diff <- mean((zST_ecdf1-rvec)^2) # h = 4: 0.03382078 || h = 1: 0.01705973
zSTGDPonly_ecdf1 <- PITtest_env$PITtest(PitSTGDPonly_OOS, rvec)
CnSS_model_GDPonly <- 1 - mean((zSTGDPonly_ecdf1 - seq(0, 1, length.out = length(zSTGDPonly_ecdf1)))^2) / var(zSTGDPonly_ecdf1) # h = 4: 0.92477 || h = 1:  0.9237593 is the value. 1 is perfect calibrated
mean_squared_diff_GDPonly <- mean((zSTGDPonly_ecdf1-rvec)^2) # h = 4: 0.008269042 || h = 1: 0.008313394
if (h == 1) {
# Use asymptotic 5% critical value from Rossi and Sekhposyan (2017): 1.34
kappa <- 1.34
kappaGDPonly <- 1.34
} else if (h == 4) {
# Compute bootstrapped 5% critical values
PITs <- cbind(PitST_OOS, PitSTGDPonly_OOS)
PITs <- PITs[(jtFirstOOS + h):nrow(PITs), , drop = FALSE]
#testcritvalues <- matrix(NA, nrow = 1, ncol = 2, dimnames = list(NULL, c('GDP and NFCI', 'GDP only')))
testcritvalues <- array(NA, dim = c(2, 3, 2))
for (i in 1:2) {
testcritvalues[,, i] <- round(rstestboot_env$rstestboot(PITs[, i])$critvalues[2] * 100) / 100
}
kappa <- testcritvalues[1, 2, 1] #different from Matlab due to seed in CVfinalbootstrapInoue
kappaGDPonly <- testcritvalues[1, 2, 2] #different from Matlab due to seed in CVfinalbootstrapInoue
}
# Plot PIT for full quantile regression vs. quantile regression with GDP only
plot(rvec, zST_ecdf1, type = 'l', col = 'blue', xlab = '??', ylab = 'Empirical CDF')
lines(rvec, zSTGDPonly_ecdf1, type = 'l', col = 'red')
P <- sum(!is.na(PitST_OOS)) #correct for both h = 1 and h = 4
lines(rvec, rvec - (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec, rvec + (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec, rvec , col = 'black',lty=2)
legend('bottomright', legend = c('GDP and NFCI', 'GDP only', 'Theoretical and 5% Critical Values'), cex = 0.5,fill = c('blue', 'red', 'black'))
#For h = 4, this PIT plot is different due to usage of a seed in CVfinalbootstrapInoue, even using the same as in Matlab, different results are obtained
ScoreSTGDPonly_OOS1
ScoreSTGDPonly_OOS1 <- ScoreSTGDPonly_OOS
ScoreST_OOS1 <- ScoreST_OOS
ScoreST_OOS1
ScoreST_OOS
ScoreST_OOS1 -ScoreST_OOS1
mean(ScoreST_OOS - ScoreST_OOSpaper, na.rm = TRUE) == 0.004654713. Predictive scores of my model are better.
mean(ScoreST_OOS - ScoreST_OOSpaper, na.rm = TRUE)# == 0.004654713. Predictive scores of my model are better.
mean(ScoreST_OOS - ScoreST_OOSpaper1, na.rm = TRUE)# == 0.004654713. Predictive scores of my model are better.
mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE)# == 0.004654713. Predictive scores of my model are better.
setwd("C:/Users/Pietro/Desktop/Pietro/Politecnico/Tesi/Thesis-Code/Conformalized Quantile Regression")
--------------------------- per caricare dati salvati -----------------------
filename <- paste("ResOOS_H1", h, ".RData", sep="")
cat(paste("Loading results from file", filename, "\n"))
load(filename)
#-----------------------------------------------------------------------------------
mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE)# == 0.004654713. Predictive scores of my model are better.
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE)# == 0.4096147. Confirmed by using sum instead of mean
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
# h = 1:> sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE)# == 1.102231 Confirmed by using sum instead of mean
mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE)# == 0.04962592. Also the model with only GDP performs better
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
# h = 1:> sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 1.102231 Confirmed by using sum instead of mean
# h = 1:> mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 0.02495498 Also the model with only GDP performs better
sum(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) #== 4.367081. Confirmed by using sum instead of mean
filename <- paste("ResOOS_H", h, ".RData", sep="")
cat(paste("Loading results from file", filename, "\n"))
load(filename)
#------------------------------
PITtest_env <- new.env()
source("PITtest.r",local = PITtest_env)
rstestboot_env <- new.env()
source("rstestboot.r",local = rstestboot_env)
# Figure 10. Out-of-sample Predictions.             #CORRETTO!!!!!!!!
# (a)/(b) Quantiles
par(mar = c(3, 3, 2, 1))  # Adjust the values as needed (bottom, left, top, right)
plot(Time, YQ_OOS[, jq05], type = 'l', col = 'blue', xlab = 'Time', ylab = 'Quantiles', xlim = range(Time),ylim = c(-20,20))
lines(Time, YQ_OOS[, jq50], type = 'l', col = 'blue', lty = 2)
lines(Time, YQ_OOS[,jq95], type = 'l', col = 'blue', lty = 3)
lines(Time, YQ_IS[, jq05], type = 'l', col = 'black', lty = 1)
lines(Time, YQ_IS[, jq50], type = 'l', col = 'black', lty = 2)
lines(Time, YQ_IS[, jq95], type = 'l', col = 'black', lty = 3)
# (c)/(d) Downside Entropy        #CORRETTO!!!!!!!!!
plot(Time, LeftEntropy_OOS, type = 'l', col = 'blue',
xlab = 'Time', ylab = 'Entropy', xlim = range(Time))
lines(Time, LeftEntropy_IS, type = 'l', col = 'black', lty = 2)
# Figure 11. Out-of-sample Accuracy.   #CORRETTO!!!!!
# (a)/(b) Predictive scores
plot(Time, ScoreST_OOS, type = 'l', col = 'blue', xlab = 'Time', ylab = 'Scores')
lines(Time, ScoreSTGDPonly_OOS, type = 'l', col = 'black', lty = 2)
legend('topleft', legend = c('GDP and NFCI', 'GDP only'))
# (c)/(d): PITs
# The code below was modified from files provided by Barbara Rossi and
# Tatevik Sekhposyan implementing the specification tests for predictive
# densities described in Rossi and Sekhposyan (2017).
rvec <- seq(0, 1, by = 0.001)
zST_ecdf1 <- PITtest_env$PITtest(PitST_OOS, rvec)
CnSS_model <- 1 - mean((zST_ecdf1 - seq(0, 1, length.out = length(zST_ecdf1)))^2) / var(zST_ecdf1) # h = 4: 0.55658995 || h = 1: 0.7951442  is the value. 1 is perfect calibrated
mean_squared_diff <- mean((zST_ecdf1-rvec)^2) # h = 4: 0.03382078 || h = 1: 0.01705973
zSTGDPonly_ecdf1 <- PITtest_env$PITtest(PitSTGDPonly_OOS, rvec)
CnSS_model_GDPonly <- 1 - mean((zSTGDPonly_ecdf1 - seq(0, 1, length.out = length(zSTGDPonly_ecdf1)))^2) / var(zSTGDPonly_ecdf1) # h = 4: 0.92477 || h = 1:  0.9237593 is the value. 1 is perfect calibrated
mean_squared_diff_GDPonly <- mean((zSTGDPonly_ecdf1-rvec)^2) # h = 4: 0.008269042 || h = 1: 0.008313394
if (h == 1) {
# Use asymptotic 5% critical value from Rossi and Sekhposyan (2017): 1.34
kappa <- 1.34
kappaGDPonly <- 1.34
} else if (h == 4) {
# Compute bootstrapped 5% critical values
PITs <- cbind(PitST_OOS, PitSTGDPonly_OOS)
PITs <- PITs[(jtFirstOOS + h):nrow(PITs), , drop = FALSE]
#testcritvalues <- matrix(NA, nrow = 1, ncol = 2, dimnames = list(NULL, c('GDP and NFCI', 'GDP only')))
testcritvalues <- array(NA, dim = c(2, 3, 2))
for (i in 1:2) {
testcritvalues[,, i] <- round(rstestboot_env$rstestboot(PITs[, i])$critvalues[2] * 100) / 100
}
kappa <- testcritvalues[1, 2, 1] #different from Matlab due to seed in CVfinalbootstrapInoue
kappaGDPonly <- testcritvalues[1, 2, 2] #different from Matlab due to seed in CVfinalbootstrapInoue
}
# Plot PIT for full quantile regression vs. quantile regression with GDP only
plot(rvec, zST_ecdf1, type = 'l', col = 'blue', xlab = '??', ylab = 'Empirical CDF')
lines(rvec, zSTGDPonly_ecdf1, type = 'l', col = 'red')
P <- sum(!is.na(PitST_OOS)) #correct for both h = 1 and h = 4
lines(rvec, rvec - (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec, rvec + (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec, rvec , col = 'black',lty=2)
legend('bottomright', legend = c('GDP and NFCI', 'GDP only', 'Theoretical and 5% Critical Values'), cex = 0.5,fill = c('blue', 'red', 'black'))
#For h = 4, this PIT plot is different due to usage of a seed in CVfinalbootstrapInoue, even using the same as in Matlab, different results are obtained
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
# h = 1:> sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 1.102231 Confirmed by using sum instead of mean
# h = 1:> mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 0.02495498 Also the model with only GDP performs better
# h = 1:> sum(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) #== 2.270903 Confirmed by using sum instead of mean
plot(ecdf(ScoreST_OOS), main="ECDF Comparison", col="blue", lty=1, lwd=2)
lines(ecdf(ScoreST_OOS1), col="red", lty=2, lwd=2)
hist(ScoreST_OOS, col="blue", alpha=0.7, main="Histogram Comparison")
hist(ScoreST_OOS1, col="red", add=TRUE, alpha=0.7)
hist(ScoreST_OOS1, col="red", add=TRUE, alpha=0.5)
hist(ScoreST_OOS, col="blue", alpha=0.7, main="Histogram Comparison")
hist(ScoreST_OOS1, col="red", add=TRUE, alpha=0.5)
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
# h = 1:> sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 1.102231 Confirmed by using sum instead of mean
# h = 1:> mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 0.02495498 Also the model with only GDP performs better
# h = 1:> sum(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) #== 2.270903 Confirmed by using sum instead of mean
plot(ecdf(ScoreSTGDPonly_OOS), main="ECDF Comparison", col="blue", lty=1, lwd=2)
lines(ecdf(ScoreSTGDPonly_OOS1), col="red", lty=2, lwd=2)
legend("bottomright", legend=c("Model 1", "Model 2"), col=c("blue", "red"), lty=c(1, 2), lwd=2)
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
# h = 1:> sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 1.102231 Confirmed by using sum instead of mean
# h = 1:> mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 0.02495498 Also the model with only GDP performs better
# h = 1:> sum(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) #== 2.270903 Confirmed by using sum instead of mean
plot(ecdf(ScoreSTGDPonly_OOS), main="PS ECDF Comparison", col="blue", lty=1, lwd=2)
lines(ecdf(ScoreSTGDPonly_OOS1), col="red", lty=2, lwd=2)
legend("bottomright", legend=c("Paper", "Mio"), col=c("blue", "red"), lty=c(1, 2), lwd=2)
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
# h = 1:> sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 1.102231 Confirmed by using sum instead of mean
# h = 1:> mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 0.02495498 Also the model with only GDP performs better
# h = 1:> sum(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) #== 2.270903 Confirmed by using sum instead of mean
plot(ecdf(ScoreSTGDPonly_OOS), main="PS ECDF Comparison", col="blue", lty=1, lwd=2)
lines(ecdf(ScoreSTGDPonly_OOS1), col="red", lty=2, lwd=2)
legend("bottomright", legend=c("Mio", "Paper"), col=c("blue", "red"), lty=c(1, 2), lwd=2)
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
# h = 1:> sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 1.102231 Confirmed by using sum instead of mean
# h = 1:> mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 0.02495498 Also the model with only GDP performs better
# h = 1:> sum(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) #== 2.270903 Confirmed by using sum instead of mean
plot(ecdf(ScoreSTGDPonly_OOS), main="PS ECDF Comparison GDPonly", col="blue", lty=1, lwd=2)
lines(ecdf(ScoreSTGDPonly_OOS1), col="red", lty=2, lwd=2)
legend("bottomright", legend=c("Mio", "Paper"), col=c("blue", "red"), lty=c(1, 2), lwd=2)
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
# h = 1:> sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 1.102231 Confirmed by using sum instead of mean
# h = 1:> mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 0.02495498 Also the model with only GDP performs better
# h = 1:> sum(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) #== 2.270903 Confirmed by using sum instead of mean
plot(ecdf(ScoreST_OOS), main="PS ECDF Comparison GDP+NFCI", col="blue", lty=1, lwd=2)
lines(ecdf(ScoreST_OOS1), col="red", lty=2, lwd=2)
legend("bottomright", legend=c("Mio", "Paper"), col=c("blue", "red"), lty=c(1, 2), lwd=2)
setwd("C:/Users/Pietro/Desktop/Pietro/Politecnico/Tesi/Thesis-Code/R version of Adrian's code")
h <-4
filename <- paste("ResOOS_H", h, ".RData", sep="")
cat(paste("Loading results from file", filename, "\n"))
load(filename)
PITtest_env <- new.env()
source("PITtest.r",local = PITtest_env)
rstestboot_env <- new.env()
source("rstestboot.r",local = rstestboot_env)
# (a)/(b) Quantiles
par(mar = c(3, 3, 2, 1))  # Adjust the values as needed (bottom, left, top, right)
ScoreST_OOS1 <- ScoreST_OOS
ScoreSTGDPonly_OOS1 <- ScoreSTGDPonly_OOS
setwd("C:/Users/Pietro/Desktop/Pietro/Politecnico/Tesi/Thesis-Code/Conformalized Quantile Regression")
filename <- paste("ResOOS_H1", h, ".RData", sep="")
cat(paste("Loading results from file", filename, "\n"))
load(filename)
filename <- paste("ResOOS_H2", h, ".RData", sep="")
cat(paste("Loading results from file", filename, "\n"))
load(filename)
PITtest_env <- new.env()
source("PITtest.r",local = PITtest_env)
rstestboot_env <- new.env()
source("rstestboot.r",local = rstestboot_env)
# (a)/(b) Quantiles
par(mar = c(3, 3, 2, 1))  # Adjust the values as needed (bottom, left, top, right)
plot(Time, YQ_OOS[, jq05], type = 'l', col = 'blue', xlab = 'Time', ylab = 'Quantiles', xlim = range(Time),ylim = c(-20,20))
lines(Time, YQ_OOS[, jq50], type = 'l', col = 'blue', lty = 2)
lines(Time, YQ_OOS[,jq95], type = 'l', col = 'blue', lty = 3)
lines(Time, YQ_IS[, jq05], type = 'l', col = 'black', lty = 1)
lines(Time, YQ_IS[, jq50], type = 'l', col = 'black', lty = 2)
lines(Time, YQ_IS[, jq95], type = 'l', col = 'black', lty = 3)
# (c)/(d) Downside Entropy
plot(Time, LeftEntropy_OOS, type = 'l', col = 'blue',
xlab = 'Time', ylab = 'Entropy', xlim = range(Time))
lines(Time, LeftEntropy_IS, type = 'l', col = 'black', lty = 2)
# Figure 11. Out-of-sample Accuracy.
# (a)/(b) Predictive scores
plot(Time, ScoreST_OOS, type = 'l', col = 'blue', xlab = 'Time', ylab = 'Scores')
lines(Time, ScoreSTGDPonly_OOS, type = 'l', col = 'black', lty = 2)
legend('topleft', legend = c('GDP and NFCI', 'GDP only'))
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
# h = 1:> sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 1.102231 Confirmed by using sum instead of mean
# h = 1:> mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 0.02495498 Also the model with only GDP performs better
# h = 1:> sum(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) #== 2.270903 Confirmed by using sum instead of mean
plot(ecdf(ScoreST_OOS), main="PS ECDF Comparison GDP+NFCI", col="blue", lty=1, lwd=2)
lines(ecdf(ScoreST_OOS1), col="red", lty=2, lwd=2)
legend("bottomright", legend=c("Mio", "Paper"), col=c("blue", "red"), lty=c(1, 2), lwd=2)
# (c)/(d): PITs
# The code below was modified from files provided by Barbara Rossi and
# Tatevik Sekhposyan implementing the specification tests for predictive
# densities described in Rossi and Sekhposyan (2017).
rvec <- seq(0, 1, by = 0.001)
zST_ecdf <- PITtest_env$PITtest(PitST_OOS, rvec)
CnSS_model <- 1 - mean((zST_ecdf - seq(0, 1, length.out = length(zST_ecdf)))^2) / var(zST_ecdf) # h = 4: 0.67701012121 || h = 1: 0.9366  is the value. 1 is perfect calibrated
mean_squared_diff <- mean((zST_ecdf-rvec)^2) # h = 4: 0.02338473 || h = 1: 0.005109203
zSTGDPonly_ecdf <- PITtest_env$PITtest(PitSTGDPonly_OOS, rvec)
CnSS_model_GDPonly <- 1 - mean((zSTGDPonly_ecdf - seq(0, 1, length.out = length(zSTGDPonly_ecdf)))^2) / var(zSTGDPonly_ecdf) # h = 4: 0.872747225 || h = 1: 0.95697 is the value. 1 is perfect calibrated
mean_squared_diff_GDPonly <- mean((zSTGDPonly_ecdf-rvec)^2) # h = 4: 0.009966178 || h = 1: 0.003439986
if (h == 1) {
# Use asymptotic 5% critical value from Rossi and Sekhposyan (2017): 1.34
kappa <- 1.34
kappaGDPonly <- 1.34
} else if (h == 4) {
# Compute bootstrapped 5% critical values
PITs <- cbind(PitST_OOS, PitSTGDPonly_OOS)
PITs <- PITs[(jtFirstOOS + h):nrow(PITs), , drop = FALSE]
#testcritvalues <- matrix(NA, nrow = 1, ncol = 2, dimnames = list(NULL, c('GDP and NFCI', 'GDP only')))
testcritvalues <- array(NA, dim = c(2, 3, 2))
for (i in 1:2) {
testcritvalues[,, i] <- round(rstestboot_env$rstestboot(PITs[, i])$critvalues[2] * 100) / 100
}
kappa <- testcritvalues[1, 2, 1] #different from Matlab due to seed in CVfinalbootstrapInoue
kappaGDPonly <- testcritvalues[1, 2, 2] #different from Matlab due to seed in CVfinalbootstrapInoue
}
# Plot PIT for full quantile regression vs. quantile regression with GDP only
plot(rvec, zST_ecdf, type = 'l', col = 'blue', xlab = '??', ylab = 'Empirical CDF')
lines(rvec, zSTGDPonly_ecdf, type = 'l', col = 'red')
P <- sum(!is.na(PitST_OOS)) #correct for both h = 1 and h = 4
lines(rvec, rvec - (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec, rvec + (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec,rvec , col = 'black',lty=2)
legend('bottomright', legend = c('GDP and NFCI', 'GDP only', 'Theoretical and 5% Critical Values'), cex = 0.5,fill = c('blue', 'red', 'black'))
#For h = 4, this PIT plot is different due to usage of a seed in CVfinalbootstrapInoue, even using the same as in Matlab, different results are obtained
#For h = 4, this PIT plot is different due to usage of a seed in CVfinalbootstrapInoue, even using the same as in Matlab, different results are obtained
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
# h = 1:> sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 1.102231 Confirmed by using sum instead of mean
# h = 1:> mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 0.02495498 Also the model with only GDP performs better
# h = 1:> sum(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) #== 2.270903 Confirmed by using sum instead of mean
plot(ecdf(ScoreSTGDPonly_OOS), main="PS ECDF Comparison GDPonly", col="blue", lty=1, lwd=2)
lines(ecdf(ScoreSTGDPonly_OOS1), col="red", lty=2, lwd=2)
legend("bottomright", legend=c("Mio", "Paper"), col=c("blue", "red"), lty=c(1, 2), lwd=2)
