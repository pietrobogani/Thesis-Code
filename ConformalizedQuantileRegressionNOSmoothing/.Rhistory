kappa <- testcritvalues[1, 2, 1] #different from Matlab due to seed in CVfinalbootstrapInoue
kappaGDPonly <- testcritvalues[1, 2, 2] #different from Matlab due to seed in CVfinalbootstrapInoue
}
# Plot PIT for full quantile regression vs. quantile regression with GDP only
plot(rvec, zST_ecdf, type = 'l', col = 'blue', xlab = '??', ylab = 'Empirical CDF')
lines(rvec, zSTGDPonly_ecdf, type = 'l', col = 'red')
P <- sum(!is.na(PitST_OOS)) #correct for both h = 1 and h = 4
lines(rvec, rvec - (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec, rvec + (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec,rvec , col = 'black',lty=2)
# (c)/(d) Downside Entropy
plot(Time, LeftEntropy_OOS, type = 'l', col = 'blue',
xlab = 'Time', ylab = 'Entropy', xlim = range(Time))
PSTunc_IS[jt + h, ]
DO LA DISTRIB EMPIRICA DEI QUANTILI, SOTTO IL QUANTILE MINIMO E SOPRA IL MASSIMO LA FUNZIONE dst RITORNA SEMPRE 0
#    POTREBBE ESSERE UN PROBLEMA E VA INVESTIGATO. PER ORA HO CERCATO DI ARGINARE USANDO COME QUANTILE MINIMO 0.01 (E NON 0.05)
#    E USANDO COME MASSIMO 0.99 (E NON 0.95). HA MIGLIORATO UN POCO MA NON RISOLTO
# 2) LA STESSA COSA ACCADE CON LA FUNZIONE pst.
# 3) QUESTO CAUSA PROBLEMI ANCHE SULLA ENTROPY. INFATTI HO VALORI CHE SULLA CONDITIONAL E/O SULLA UNCONDITIONAL HANNO PROBABILITà 0.
#    DOVENDO FARNE IL log, CAUSA GROSSI PROBLEMI. RISOLVO SEMPLICEMENTE TENENDO SOLO GLI INDICI IN CUI SIA COND CHE UNCOND SONO >0
# CAMBI FATTI
# 1) GRIGLIA MOLTO PIù FITTA DI QQ
# Install and load necessary packages
library(quantreg)
library(lubridate)
library(pracma)
library(readxl)
library(sn)
library(parallel)
# Clear workspace
rm(list = ls())
#IMPLEMENTATION OF QST, DST, PST, THAT I NEED SINCE I DON'T DO SMOOTHING ANYMORE
pst <- function(X, QQ, qqtarg) { #I consider it a step a function
sapply(X, function(x) {
if (length(qqtarg[qqtarg <= x]) == 0) {
return(0) # If x is less than the smallest quantile value, the cumulative probability is 0
}
if (x > max(qqtarg, na.rm = TRUE)) {
return(1) # If x is greater than all qqtarg, return the maximum cumulative probability
}
max_value_not_exceeding_x <- max(qqtarg[qqtarg <= x], na.rm = TRUE)
quantile_index <- which(qqtarg == max_value_not_exceeding_x)
return(QQ[min(quantile_index)])
})
}
dst <- function(X, QQ, qqTarg) { #I consider in between quantiles probabilities is uniformly distributed
# Function to find density for a single point
find_density <- function(x) {
for (i in 1:(length(qqTarg) - 1)) {
if (x >= qqTarg[i] && x <= qqTarg[i + 1]) {
# Calculate the density based on the assumption of a uniform distribution
# between the two quantiles
density <- 1 / (qqTarg[i + 1] - qqTarg[i]) * (QQ[i + 1] - QQ[i])
return(density)
}
}
# If x is outside the range of given QQ, return 0 as the density
return(0)
}
# Apply find_density to each element in X
densities <- sapply(X, find_density)
return(densities)
}
qst <- function(QQ, qqTarg) { # qst is always called giving in input QQ. Without smoothing, the estimated quantiles are exactly qqTarg!
if (length(QQ)== length(qqTarg)) {
return(qqTarg)
}
else
cat("wrong dimensions")
}
# Set forecast horizon (run script separately for h = 1 and h = 4)
h <- 1
loadsavedresults = FALSE; # If I ran code already results are stored in ResOOS_H11 and ResOOS_H14, in that case, set = TRUE
# Graphics settings - R's graphics system differs from MATLAB's, so some modifications are necessary
par(mfrow = c(1, 1))  # Reset plot window to single pane
# Load data
file_path <- "DataVulnerabilityAppendix.xls"
# Read the file
data <- read_excel(file_path)
data<-data[,1:3]
# Filter data for 1973Q1-2015Q4
colnames(data)[1] <- "Time"
data$Time <- as.Date(data$Time)
# Subset the data
data <- data[data$Time >= as.Date("1973-01-01") & data$Time <= as.Date("2015-10-01"), ]
X <- data[,2:3]
Time <- data$Time
# Set forecast settings
#QQ <- seq(0.05, 0.95, by = 0.05)
QQ <- seq(0.01, 0.99, by = 0.01) #Let's try a much more fine grid
#QQ <- c(0.01, QQ, 0.99) # dst function returns too many zeros. This should help
deltaYY <- 0.1
YY <- seq(-20, 20, by = deltaYY)
jtFirstOOS <- which(year(data$Time) == 1993 & month(data$Time) == 1)
indices <- which(QQ %in% c(0.05, 0.25, 0.5, 0.75, 0.95))
jq05 <- indices[1]
jq25 <- indices[2]
jq50 <- indices[3]
jq75 <- 15 #couldn't automatically translate from MATLAB, I set it manually
jq95 <- 19 #couldn't automatically translate from MATLAB, I set it manually
# Construct average growth rates
y <- X$A191RL1Q225SBEA
Yh <- matrix(0, nrow=length(y), ncol=4)
Yh <- filter(y, rep(1/h, h), sides=1) #If h = 1, y = Yh
if (h>1){
Yh[1:(h-1)] <- NA
}
#Construct matrices of regressors
Z <- cbind(1, X[,2], y)
ZGDPonly <- cbind(1, y)
Z <-as.matrix(Z)
# Get length of Time and QQ/YY
len_time <- length(data$Time)
len_qq <- length(QQ)
len_yy <- length(YY)
# Initialize matrices to store forecasts
{
# Raw QQ
YQ_NaNs <- matrix(NA, len_time, len_qq)
#YQ_low_adj_IS <- YQ_NaNs #IN the original script I've just YQ_IS, because I do a point estimate of the quantile. Here I do a confidence interval estimation and I need 2
#YQ_high_adj_IS <- YQ_NaNs
YQ_low_IS <- YQ_NaNs
YQ_high_IS <- YQ_NaNs
YQ_IS <- YQ_NaNs
#YQ_low_adj_OOS <- YQ_NaNs
#YQ_high_adj_OOS <- YQ_NaNs
YQ_low_OOS <- YQ_NaNs
YQ_high_OOS <- YQ_NaNs
YQ_OOS <- YQ_NaNs
YQ_lowGDPonly_IS <- YQ_NaNs
YQ_highGDPonly_IS <- YQ_NaNs
YQGDPonly_IS <- YQ_NaNs
YQGDPonly_OOS <- YQ_NaNs
YQunc_IS <- YQ_NaNs
YQunc_OOS <- YQ_NaNs
YQunclow_IS <- YQ_NaNs
YQunchigh_IS <- YQ_NaNs
YQGDPonly_high_OOS <- YQ_NaNs
YQGDPonly_low_OOS <- YQ_NaNs
YQunclow_OOS <- YQ_NaNs
YQunchigh_OOS <- YQ_NaNs
YQunchigh_OOS <- YQ_NaNs
YQunclow_OOS <- YQ_NaNs
# PDFs (evaluated over grid)
P_NaNs <- matrix(NA, len_time, len_yy)
PST_IS <- P_NaNs
PST_OOS <- P_NaNs
PSTGDPonly_IS <- P_NaNs
PSTGDPonly_OOS <- P_NaNs
PSTunc_IS <- P_NaNs
PSTunc_OOS <- P_NaNs
# Smoothed QQ
Q_NaNs <- matrix(NA, len_time, len_qq)
QST_IS <- Q_NaNs
QST_OOS <- Q_NaNs
QSTGDPonly_IS <- Q_NaNs
QSTGDPonly_OOS <- Q_NaNs
QSTunc_IS <- Q_NaNs
QSTunc_OOS <- Q_NaNs
# CDFs (evaluated over grid)
C_NaNs <- matrix(NA, len_time, len_yy)
CST_IS <- C_NaNs
CST_OOS <- C_NaNs
CSTGDPonly_IS <- C_NaNs
CSTGDPonly_OOS <- C_NaNs
CSTunc_IS <- C_NaNs
CSTunc_OOS <- C_NaNs
# Skewed t-distribution parameters
STpar_NaNs <- matrix(NA, len_time, 4)
STpar_IS <- STpar_NaNs
STpar_OOS <- STpar_NaNs
STparGDPonly_IS <- STpar_NaNs
STparGDPonly_OOS <- STpar_NaNs
STparunc_IS <- STpar_NaNs
STparunc_OOS <- STpar_NaNs
# Predictive scores
Score_NaNs <- rep(NA, len_time)
ScoreST_IS <- Score_NaNs
ScoreST_OOS <- Score_NaNs
ScoreSTGDPonly_IS <- Score_NaNs
ScoreSTGDPonly_OOS <- Score_NaNs
ScoreSTunc_IS <- Score_NaNs
ScoreSTunc_OOS <- Score_NaNs
# Probability integral transforms
Pit_NaNs <- rep(NA, len_time)
PitST_IS <- Pit_NaNs
PitST_OOS <- Pit_NaNs
PitSTGDPonly_IS <- Pit_NaNs
PitSTGDPonly_OOS <- Pit_NaNs
PitSTunc_IS <- Pit_NaNs
PitSTunc_OOS <- Pit_NaNs
# Left entropy
Entropy_NaNs <- rep(NA, len_time)
LeftEntropy_IS <- Entropy_NaNs
LeftEntropy_OOS <- Entropy_NaNs
#Split creating I1 and I2
full_length <- length(Yh[(h + 1):length(Yh)])
#Let's do a 75-25 split
test_length <- full_length*50/100
#permuted_indices <- c(1,2,3,4,sample((h+1):length(Yh))) #if you want to permute
permuted_indices <- c(1:length(Yh)) # if you don't want to permute
Yh_perm <- Yh[permuted_indices]
Z_perm <- Z[permuted_indices,]
ZGDPonly_perm <- ZGDPonly[permuted_indices,]
Yh1 <- Yh_perm[(h+1):(h+test_length)]
Yh2 <- Yh_perm[(h+1+test_length):length(Yh)]
Z1 <- Z_perm[1:(test_length),]
Z2 <- Z_perm[(test_length+1):(length(Yh) - h),]
ZGDPonly1 <- ZGDPonly_perm[1:test_length,]
ZGDPonly2 <- ZGDPonly_perm[(test_length+1):(length(Yh) - h),]
}
if (loadsavedresults == FALSE) {
#-------------------    %% In-sample estimation of conditional QQ
for (jq in 1:length(QQ)) {
#CQR is built for intervals, not QQ. Thus I create a prediction interval of the form [-Inf, quantile X] creating a prediction interval with
#confidence X %
#------ Conformalized Quantile regression with both NFCI and GDP
YQ_low_IS[(h + 1):(h + test_length), jq] <- -Inf
b_high <- rq(Yh1 ~ Z1[,-1], tau= QQ[jq])
YQ_high_IS[(h + 1):(h + full_length-test_length), jq] <- as.vector(Z2 %*% coef(b_high))
# Initialize a vector for errors
E_i <- rep(NA, length(Yh2))
# Calculate errors for each point in the test set I2
for (i in 1:length(E_i)) {
E_i[i] <- max(YQ_low_IS[h + i, jq] - Yh2[i], Yh2[i] - YQ_high_IS[h + i, jq])
}
# Compute Q(QQ[jq])(E, I2) N.B 1 - ?? = QQ[jq]
quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
YQ_IS[(h + 1):length(Yh), jq] <- as.vector(Z[1:(length(Yh) - h),] %*% coef(b_high)) + quantile_E
#------ Conformalized Quantile regression with GDP only
YQ_lowGDPonly_IS[(h + 1):(h + test_length), jq] <- -Inf
b_highGDPonly <- rq(Yh1 ~ ZGDPonly1[,-1], tau= QQ[jq])
YQ_highGDPonly_IS[(h + 1):(h + full_length-test_length), jq] <- as.vector(ZGDPonly2 %*% coef(b_highGDPonly))
# Initialize a vector for errors
E_i <- rep(NA, length(Yh2))
# Calculate errors for each point in the test set I2
for (i in 1:length(E_i)) {
E_i[i] <- max(YQ_lowGDPonly_IS[h + i, jq] - Yh2[i], Yh2[i] - YQ_highGDPonly_IS[h + i, jq])
}
# Compute Q(QQ[jq])(E, I2) N.B 1 - ?? = QQ[jq]
quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
YQGDPonly_IS[(h + 1):length(Yh), jq] <- as.vector(ZGDPonly[1:(length(Yh) - h),] %*% coef(b_highGDPonly)) + quantile_E
#------ Unconditional QQ (conformalized quantile regression on constant)
YQunclow_IS[(h + 1):(h + test_length), jq] <- - Inf
bunc_high <- rq(Yh1 ~ 1, tau=QQ[jq])
YQunchigh_IS[(h + 1):(h + test_length), jq] <- rep(coef(bunc_high), length((h + 1):(h + test_length)))
# Initialize a vector for errors
E_i <- rep(NA, length(Yh2))
# Calculate errors for each point in the test set I2
for (i in 1:length(E_i)) {
E_i[i] <- max(YQunclow_IS[h + i, jq] - Yh2[i], Yh2[i] - YQunchigh_IS[h + i, jq])
}
# Compute Q(QQ[jq])(E, I2) N.B 1 - ?? = QQ[jq]
quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
YQunc_IS[(h + 1):length(Yh), jq] <- rep(coef(bunc_high), length(Time) - h) + quantile_E
}
#---------    %% Fit skewed-t distribution for in-sample unconditional QQ
{
# Fit skewed-t distribution for in-sample unconditional QQ
qqTarg <- YQunc_IS[nrow(YQunc_IS), ]
# Assign values to matrices based on the skewed-t fit
densities <- dst(YY, QQ, qqTarg) # PROBLEMA! visto che ho solo quantili empirici, sotto quantile 0.05 e sopra 0.95 assegno densità =0. Cosa che con smoothing non succede. Come risolvere?
replicated_matrix <- matrix(rep(densities, each = length(Time) - h), ncol = length(densities))
PSTunc_IS[(h + 1):nrow(PSTunc_IS), ] <- replicated_matrix #Giusto, ma tanti 0, forse troppi che danno problemi
densities <- qst(QQ, qqTarg) #diverso dall'originale che invece dava i quantili dopo smoothing
replicated_matrix <- matrix(rep(densities, each = length(Time) - h), ncol = length(densities))
QSTunc_IS[(h + 1):nrow(QSTunc_IS), ] <- replicated_matrix #Credo giusto
densities <- pst(YY, QQ, qqTarg)
replicated_matrix <- matrix(rep(densities, each = length(Time) - h), ncol = length(densities))
CSTunc_IS[(h + 1):nrow(CSTunc_IS), ] <- replicated_matrix #Credo sbagliato
#STparunc_IS[(h + 1):nrow(STparunc_IS), ] <- matrix(rep(c(lc, sc, sh, df), times = length(Time) - h),
#                                                   nrow = length(Time) - h,
#                                                   byrow = TRUE)
#Questo semplicemente salva i parametri, non mi serve più
ScoreSTunc_IS[(h + 1):length(Yh)] <- dst(Yh[(h + 1):length(Yh)], QQ, qqTarg)
PitSTunc_IS[(h + 1):length(Yh)] <- pst(Yh[(h + 1):length(Yh)], QQ, qqTarg)
}
#---------------------    %% Out-of-sample estimation of conditional QQ
for (jt in 1:(length(Time) - h)) {
month_val <- as.numeric(format(Time[jt], "%m"))
year_val <- as.numeric(format(Time[jt], "%Y"))
if (month_val == 1 && jt >= jtFirstOOS) { #jtFirstOOS is the date since when I start computing Out-of-sample estimations
cat(sprintf("Computing in-sample and out-of-sample predictive densities in %d", year_val), "\n")
} else {
cat(sprintf("Computing in-sample predictive densities in %d", year_val), "\n")
}
YhRealized <- Yh[jt + h]
qqTarg <- YQ_IS[jt + h, ]
# params <- QQInterpolation_env$QQInterpolation(qqTarg, QQ)  # You might need to adjust this function to return params in appropriate format for dst, qst, pst
# lc <- params$lc
# sc <- params$sc
# sh <- params$sh
# df <- params$df
PST_IS[jt + h, ] <- dst(YY, QQ, qqTarg)
QST_IS[jt + h, ] <- qst(QQ, qqTarg)
CST_IS[jt + h, ] <- pst(YY, QQ, qqTarg)
#STpar_IS[jt + h, ] <- c(lc, sc, sh, df)
ScoreST_IS[jt + h ] <- dst(YhRealized, QQ, qqTarg)
PitST_IS[jt + h ] <- pst(YhRealized, QQ, qqTarg)    # is the probability to observe a value < of YhRealized in this distribution
Temp <- PST_IS[jt + h, ] * (YY < QST_IS[jt + h, jq50])
non_zero_indexes <- (PSTunc_IS[jt + h, ]!= 0) & (PST_IS[jt + h, ] != 0)
# Create new vectors with values from those indexes
PSTunc_IS_non_zero <- PSTunc_IS[jt + h, ][non_zero_indexes]
PST_IS_non_zero <- PST_IS[jt + h, ][non_zero_indexes]
Temp_non_zero <- Temp[non_zero_indexes]
LeftEntropy_IS[jt + h] <- -sum((log(PSTunc_IS_non_zero) - log(PST_IS_non_zero)) * Temp * deltaYY)
# Similar computations for GDP only
qqTarg_GDPonly <- YQGDPonly_IS[jt + h, ]
# params <- QQInterpolation_env$QQInterpolation(qqTarg_GDPonly, QQ)
# lc <- params$lc
# sc <- params$sc
# sh <- params$sh
# df <- params$df
PSTGDPonly_IS[jt + h, ] <- dst(YY, QQ, qqTarg)
QSTGDPonly_IS[jt + h, ] <- qst(QQ, qqTarg)
CSTGDPonly_IS[jt + h, ] <- pst(YY, QQ, qqTarg)
#STparGDPonly_IS[jt + h, ] <- c(lc, sc, sh, df)
ScoreSTGDPonly_IS[jt + h] <- dst(YhRealized, QQ, qqTarg)
PitSTGDPonly_IS[jt + h] <- pst(YhRealized, QQ, qqTarg) # is the probability to observe a value < of YhRealized in this distribution
if (jt >= jtFirstOOS) {
if (month(Time[jt]) == 1) {
cat(sprintf("Now computing the real-time predictive densities in %d", year(Time[jt])), "\n")
}
for (jq in 1:length(QQ)) {
#------- Conformalized Quantile Regression with both NFCI and GDP, out-of-sample
#Split creating I1 and I2
full_length <- length(Yh[(h + 1):jt])
test_length = full_length*50/100
Yh1 <- Yh[(h+1):(h+test_length)]
Yh2 <- Yh[(h+1+test_length):jt] #Yh1 and Yh2 correctly have same dimension
Z1 <- Z[1:test_length,]
Z2 <- Z[(test_length+1):(jt - h),] #Z1 and Z2 correctly have same dimension
ZGDPonly1 <- ZGDPonly[1:test_length,]
ZGDPonly2 <- ZGDPonly[(test_length+1):(jt - h),]
#b_low <- rq(Yh1 ~ Z1[,-1], 0.01)
YQ_low_OOS[(h + 1):(h + test_length), jq] <- -Inf #as.vector(Z2 %*% coef(b_low)) #at each iteration of jt, it will be overwritten. But it's fine, we just need to extract jt + h row later on
b_high <- rq(Yh1 ~ Z1[,-1], tau= QQ[jq]) #Train on I1
YQ_high_OOS[(h + 1):(h + full_length-test_length), jq] <- as.vector(Z2 %*% coef(b_high)) #Evaluate on I2
# Initialize a vector for errors
E_i <- rep(NA, length(Yh2))
# Calculate errors for each point in the test set I2
for (i in 1:length(E_i)) {
E_i[i] <- max(YQ_low_OOS[h + i, jq] - Yh2[i], Yh2[i] - YQ_high_OOS[h + i, jq])
}
# Compute Q(1-alpha)(E, I2)
quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
# YQ_low_adj_OOS[jt + h, jq] <- Z[jt,] %*% coef(b_low) - quantile_E
YQ_OOS[jt + h, jq] <- Z[jt,] %*% coef(b_high) + quantile_E
#------- Quantile regression with GDP only, out-of-sample
YQGDPonly_low_OOS[(h + 1):(h + test_length), jq] <- -Inf
bGDPonly_high <- rq(Yh1 ~ ZGDPonly1[,-1], tau= QQ[jq]) #Train on I1
YQGDPonly_high_OOS[(h + 1):(h + full_length-test_length), jq] <- as.vector(ZGDPonly2 %*% coef(bGDPonly_high)) #Evaluate on I2
# Initialize a vector for errors
E_i <- rep(NA, length(Yh2))
# Calculate errors for each point in the test set I2
for (i in 1:length(E_i)) {
E_i[i] <- max(YQGDPonly_low_OOS[h + i, jq] - Yh2[i], Yh2[i] - YQGDPonly_high_OOS[h + i, jq])
}
# Compute Q(1-alpha)(E, I2)
quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
# YQ_low_adj_OOS[jt + h, jq] <- Z[jt,] %*% coef(b_low) - quantile_E
YQGDPonly_OOS[jt + h, jq] <- ZGDPonly[jt,] %*% coef(bGDPonly_high) + quantile_E
#------- Quantile regression with Unconditional QQ, out-of-sample
YQunclow_OOS[(h + 1):(h + test_length), jq] <- - Inf #rep(coef(bunc_low), length((h + 1):(h + full_length/2)))
bunc_high <- rq(Yh1 ~ 1, tau=QQ[jq])
YQunchigh_OOS[(h + 1):(h + test_length), jq] <- rep(coef(bunc_high), length((h + 1):(h + test_length)))
# Initialize a vector for errors
E_i <- rep(NA, length(Yh2))
# Calculate errors for each point in the test set I2
for (i in 1:length(E_i)) {
E_i[i] <- max(YQunclow_OOS[h + i, jq] - Yh2[i], Yh2[i] - YQunchigh_OOS[h + i, jq])
}
# Compute Q(1-??)(E, I2)
quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
YQunc_OOS[jt + h, jq] <- coef(bunc_high) + quantile_E
}
# Fit skewed-t distribution for quantile regression with NFCI and GDP, out-of-sample
#params <- QQInterpolation_env$QQInterpolation(YQ_OOS[jt + h, ], QQ) #YQ_OOS[jt + h, ] is the new qqTarg!
PST_OOS[jt + h, ] <- dst(YY, QQ, YQ_OOS[jt + h, ])
QST_OOS[jt + h, ] <- qst(QQ, YQ_OOS[jt + h, ])
CST_OOS[jt + h, ] <- pst(YY, QQ, YQ_OOS[jt + h, ])
#STpar_OOS[jt + h, ] <- c(params$lc, params$sc, params$sh, params$df)
ScoreST_OOS[jt + h] <- dst(YhRealized, QQ, YQ_OOS[jt + h, ])
PitST_OOS[jt + h] <- pst(YhRealized, QQ, YQ_OOS[jt + h, ]) # is the probability to observe a value < of YhRealized in this distribution
# Fit skewed-t distribution for quantile regression with GDP only, out-of-sample
#params_GDPonly <- QQInterpolation_env$QQInterpolation(YQGDPonly_OOS[jt + h, ], QQ)
PSTGDPonly_OOS[jt + h, ] <- dst(YY, QQ, YQGDPonly_OOS[jt + h, ])
QSTGDPonly_OOS[jt + h, ] <- qst(QQ, YQGDPonly_OOS[jt + h, ])
CSTGDPonly_OOS[jt + h, ] <- pst(YY, QQ, YQGDPonly_OOS[jt + h, ])
#STparGDPonly_OOS[jt + h, ] <- c(params_GDPonly$lc, params_GDPonly$sc, params_GDPonly$sh, params_GDPonly$df)
ScoreSTGDPonly_OOS[jt + h] <- dst(YhRealized, QQ, YQGDPonly_OOS[jt + h, ])
PitSTGDPonly_OOS[jt + h] <- pst(YhRealized, QQ, YQGDPonly_OOS[jt + h, ]) # is the probability to observe a value < of YhRealized in this distribution
# Fit skewed t-distribution for unconditional QQ, out-of-sample
#params_unc <- QQInterpolation_env$QQInterpolation(YQunc_OOS[jt + h, ], QQ)
PSTunc_OOS[jt + h, ] <- dst(YY, QQ, YQunc_OOS[jt + h, ])
QSTunc_OOS[jt + h, ] <- qst(QQ, YQunc_OOS[jt + h, ])
CSTunc_OOS[jt + h, ] <- pst(YY, QQ, YQunc_OOS[jt + h, ])
#STparunc_OOS[jt + h, ] <- c(params_unc$lc, params_unc$sc, params_unc$sh, params_unc$df)
ScoreSTunc_OOS[jt + h] <- dst(YhRealized, QQ, YQunc_OOS[jt + h, ])
PitSTunc_OOS[jt + h] <- pst(YhRealized, QQ, YQunc_OOS[jt + h, ]) # is the probability to observe a value < of YhRealized in this distribution
# Compute entropy for skewed t-distribution from quantile regression with GDP and NFCI, out-of-sample
Temp <- PST_OOS[jt + h, ] * (YY < QST_OOS[jt + h, jq50])
non_zero_indexes <- (PSTunc_OOS[jt + h, ] != 0) & (PST_OOS[jt + h, ] != 0)
# Create new vectors with values from those indexes
PSTunc_OOS_non_zero <- PSTunc_OOS[jt + h, ][non_zero_indexes]
PST_OOS_non_zero <- PST_OOS[jt + h, ][non_zero_indexes]
Temp_non_zero <- Temp[non_zero_indexes]
LeftEntropy_OOS[jt + h] <- -sum((log(PSTunc_OOS_non_zero) - log(PST_OOS_non_zero)) * Temp_non_zero * deltaYY)
}
}
#
#
# filename <- paste("ResOOS_H", h, "_75-25.RData", sep="")
# cat(paste("Saving results to file", filename, "\n"))
#
# # Save all the variables to the .RData file
# save(
#   YQ_IS,      YQ_OOS,      YQGDPonly_IS,      YQGDPonly_OOS,      YQunc_IS,      YQunc_OOS,
#   PST_IS,     PST_OOS,     PSTGDPonly_IS,     PSTGDPonly_OOS,     PSTunc_IS,     PSTunc_OOS,
#   QST_IS,     QST_OOS,     QSTGDPonly_IS,     QSTGDPonly_OOS,     QSTunc_IS,     QSTunc_OOS,
#   CST_IS,     CST_OOS,     CSTGDPonly_IS,     CSTGDPonly_OOS,     CSTunc_IS,     CSTunc_OOS,
#   STpar_IS,   STpar_OOS,   STparGDPonly_IS,   STparGDPonly_OOS,   STparunc_IS,   STparunc_OOS,
#   ScoreST_IS, ScoreST_OOS, ScoreSTGDPonly_IS, ScoreSTGDPonly_OOS, ScoreSTunc_IS, ScoreSTunc_OOS,
#   PitST_IS,   PitST_OOS,   PitSTGDPonly_IS,   PitSTGDPonly_OOS,   PitSTunc_IS,   PitSTunc_OOS,
#   LeftEntropy_IS, LeftEntropy_OOS,
#   file=filename
# )
#
}
#
#
# #-------------------------------- per caricare dati salvati -----------------------
#
# filename <- paste("ResOOS_H", h, "_50-50.RData", sep="")
# cat(paste("Loading results from file", filename, "\n"))
#
# load(filename)
#
# #-----------------------------------------------------------------------------------
PITtest_env <- new.env()
source("PITtest.r",local = PITtest_env)
rstestboot_env <- new.env()
source("rstestboot.r",local = rstestboot_env)
# Figure 10. Out-of-sample Predictions.
# (a)/(b) QQ
par(mar = c(3, 3, 2, 1))  # Adjust the values as needed (bottom, left, top, right)
plot(Time, YQ_OOS[, jq05], type = 'l', col = 'blue', xlab = 'Time', ylab = 'QQ', xlim = range(Time),ylim = c(-20,20))
lines(Time, YQ_OOS[, jq50], type = 'l', col = 'blue', lty = 2)
lines(Time, YQ_OOS[,jq95], type = 'l', col = 'blue', lty = 3)
lines(Time, YQ_IS[, jq05], type = 'l', col = 'black', lty = 1)
lines(Time, YQ_IS[, jq50], type = 'l', col = 'black', lty = 2)
lines(Time, YQ_IS[, jq95], type = 'l', col = 'black', lty = 3)
# (c)/(d) Downside Entropy
plot(Time, LeftEntropy_OOS, type = 'l', col = 'blue',
xlab = 'Time', ylab = 'Entropy', xlim = range(Time))
lines(Time, LeftEntropy_IS, type = 'l', col = 'black', lty = 2)
# Figure 11. Out-of-sample Accuracy.
# (a)/(b) Predictive scores
plot(Time, ScoreST_OOS, type = 'l', col = 'blue', xlab = 'Time', ylab = 'Scores')
lines(Time, ScoreSTGDPonly_OOS, type = 'l', col = 'black', lty = 2)
legend('topleft', legend = c('GDP and NFCI', 'GDP only'))
# h = 4:> mean(ScoreST_OOS - ScoreST_OOSpaper, na.rm = TRUE) == 0.004654713. Predictive scores of my model are better.
# h = 4:> sum(ScoreST_OOS - ScoreST_OOSpaper, na.rm = TRUE) == 0.4096147. Confirmed by using sum instead of mean
# h = 4:> mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 0.04962592. Also the model with only GDP performs better
# h = 4:> sum(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 4.367081. Confirmed by using sum instead of mean
# h = 1:> mean(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better.
# h = 1:> sum(ScoreST_OOS - ScoreST_OOS1, na.rm = TRUE) == 1.102231 Confirmed by using sum instead of mean
# h = 1:> mean(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) == 0.02495498 Also the model with only GDP performs better
# h = 1:> sum(ScoreSTGDPonly_OOS - ScoreSTGDPonly_OOS1, na.rm = TRUE) #== 2.270903 Confirmed by using sum instead of mean
plot(ecdf(ScoreSTGDPonly_OOS), main="PS ECDF Comparison GDPonly", col="blue", lty=1, lwd=2)
lines(ecdf(ScoreSTGDPonly_OOS1), col="red", lty=2, lwd=2)
legend("bottomright", legend=c("Mio", "Paper"), col=c("blue", "red"), lty=c(1, 2), lwd=2)
# (c)/(d): PITs
# The code below was modified from files provided by Barbara Rossi and
# Tatevik Sekhposyan implementing the specification tests for predictive
# densities described in Rossi and Sekhposyan (2017).
rvec <- seq(0, 1, by = 0.001)
zST_ecdf <- PITtest_env$PITtest(PitST_OOS, rvec)
CnSS_model <- 1 - mean((zST_ecdf - seq(0, 1, length.out = length(zST_ecdf)))^2) / var(zST_ecdf) # h = 4: 0.67701012121 || h = 1: 0.9366  is the value. 1 is perfect calibrated
mean_squared_diff <- mean((zST_ecdf-rvec)^2) # h = 4: 0.02338473 || h = 1: 0.005109203
zSTGDPonly_ecdf <- PITtest_env$PITtest(PitSTGDPonly_OOS, rvec)
CnSS_model_GDPonly <- 1 - mean((zSTGDPonly_ecdf - seq(0, 1, length.out = length(zSTGDPonly_ecdf)))^2) / var(zSTGDPonly_ecdf) # h = 4: 0.872747225 || h = 1: 0.95697 is the value. 1 is perfect calibrated
mean_squared_diff_GDPonly <- mean((zSTGDPonly_ecdf-rvec)^2) # h = 4: 0.009966178 || h = 1: 0.003439986
if (h == 1) {
# Use asymptotic 5% critical value from Rossi and Sekhposyan (2017): 1.34
kappa <- 1.34
kappaGDPonly <- 1.34
} else if (h == 4) {
# Compute bootstrapped 5% critical values
PITs <- cbind(PitST_OOS, PitSTGDPonly_OOS)
PITs <- PITs[(jtFirstOOS + h):nrow(PITs), , drop = FALSE]
#testcritvalues <- matrix(NA, nrow = 1, ncol = 2, dimnames = list(NULL, c('GDP and NFCI', 'GDP only')))
testcritvalues <- array(NA, dim = c(2, 3, 2))
for (i in 1:2) {
testcritvalues[,, i] <- round(rstestboot_env$rstestboot(PITs[, i])$critvalues[2] * 100) / 100
}
kappa <- testcritvalues[1, 2, 1] #different from Matlab due to seed in CVfinalbootstrapInoue
kappaGDPonly <- testcritvalues[1, 2, 2] #different from Matlab due to seed in CVfinalbootstrapInoue
}
# Plot PIT for full quantile regression vs. quantile regression with GDP only
plot(rvec, zST_ecdf, type = 'l', col = 'blue', xlab = '??', ylab = 'Empirical CDF')
lines(rvec, zSTGDPonly_ecdf, type = 'l', col = 'red')
P <- sum(!is.na(PitST_OOS)) #correct for both h = 1 and h = 4
lines(rvec, rvec - (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec, rvec + (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec,rvec , col = 'black',lty=2)
legend('bottomright', legend = c('GDP and NFCI', 'GDP only', 'Theoretical and 5% Critical Values'), cex = 0.5,fill = c('blue', 'red', 'black'))
#For h = 4, this PIT plot is different due to usage of a seed in CVfinalbootstrapInoue, even using the same as in Matlab, different results are obtained
# (c)/(d) Downside Entropy
plot(Time, LeftEntropy_OOS, type = 'l', col = 'blue',
xlab = 'Time', ylab = 'Entropy', xlim = range(Time))
lines(Time, LeftEntropy_IS, type = 'l', col = 'black', lty = 2)
