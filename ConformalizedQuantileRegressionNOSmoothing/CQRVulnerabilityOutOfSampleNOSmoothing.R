# PROBLEMI DEL NON AVERE LO SMOOTHING:
# 1) USANDO LA DISTRIB EMPIRICA DEI QUANTILI, SOTTO IL QUANTILE MINIMO E SOPRA IL MASSIMO LA FUNZIONE dst RITORNA SEMPRE 0
#    POTREBBE ESSERE UN PROBLEMA E VA INVESTIGATO. PER ORA HO CERCATO DI ARGINARE USANDO COME QUANTILE MINIMO 0.01 (E NON 0.05)
#    E USANDO COME MASSIMO 0.99 (E NON 0.95). HA MIGLIORATO UN POCO MA NON RISOLTO
# 2) LA STESSA COSA ACCADE CON LA FUNZIONE pst.
# 3) QUESTO CAUSA PROBLEMI ANCHE SULLA ENTROPY. INFATTI HO VALORI CHE SULLA CONDITIONAL E/O SULLA UNCONDITIONAL HANNO PROBABILITà 0.
#    DOVENDO FARNE IL log, CAUSA GROSSI PROBLEMI. RISOLVO SEMPLICEMENTE TENENDO SOLO GLI INDICI IN CUI SIA COND CHE UNCOND SONO >0
# 4) PER MANCANZA DI DATI A VOLTE DUE qqTarg ADIACENTI SONO UGUALI! QUESTO FA Sì CHE  dst RESTITUISCA VALORI < 1. IN QUEI CASI ALLARGO
#    L'INTERVALLO CONSIDERANDO I QUANTILI PRIMA E DOPO IN MODO ITERATIVO. 
# 5) SIMILE AL PUNTO 4). A VOLTE GLI INTERVALLI SONO PICCOLISSIMI, E RESTITUISCE ANCORA >1. USO LA STESSA TECNICA PER RISOLVERE
# 6) dst SEMBRA LA PIU' PROBLEMATICA


# CAMBI FATTI
# 1) GRIGLIA MOLTO PIù FITTA DI QQ


# Install and load necessary packages
library(quantreg)
library(lubridate)
library(pracma)
library(readxl)
library(sn)
library(parallel)

# Clear workspace 
rm(list = ls())

#IMPLEMENTATION OF QST, DST, PST, THAT I NEED SINCE I DON'T DO SMOOTHING ANYMORE

 pst <- function(X, QQ, qqtarg) { #Restituisce il valore della funzione di ripartizione in X
   #I consider it a step a function
   
   sapply(X, function(x) {
     if (length(qqtarg[qqtarg <= x]) == 0) {
       return(0) # If x is less than the smallest quantile value, the cumulative probability is 0
     }
     
     if (x > max(qqtarg, na.rm = TRUE)) {
       return(1) # If x is greater than all qqtarg, return the maximum cumulative probability
     }
     
     max_value_not_exceeding_x <- max(qqtarg[qqtarg <= x], na.rm = TRUE)
     quantile_index <- which(qqtarg == max_value_not_exceeding_x)
     
     
     return(QQ[min(quantile_index)])
   })
 }

 # pst <- function(X, QQ, qqtarg) { # Interpolating assuming uniform distribution between quantiles, other version of the function
 #   
 #   sapply(X, function(x) {
 #     if (x <= min(qqtarg)) {
 #       return(0) # If x is less than the smallest quantile value, the cumulative probability is 0
 #     }
 #     
 #     if (x >= max(qqtarg)) {
 #       return(1) # If x is greater than all qqtarg, return the maximum cumulative probability
 #     }
 #     
 #     # Find the largest quantile less than or equal to x
 #     lower_index <- max(which(qqtarg <= x))
 #     if (x == qqtarg[lower_index]) {
 #       return(QQ[lower_index])
 #     } else {
 #       # Linear interpolation for x between two consecutive quantiles
 #       upper_index <- lower_index + 1
 #       proportion <- (x - qqtarg[lower_index]) / (qqtarg[upper_index] - qqtarg[lower_index])
 #       interpolated_quantile <- QQ[lower_index] + proportion * (QQ[upper_index] - QQ[lower_index])
 #       return(interpolated_quantile)
 #     }
 #   })
 # }






# 
# dst <- function(X, QQ, qqTarg) { #I consider in between quantiles probabilities is uniformly distributed
#   
#   # Function to find density for a single point
#   find_density <- function(x) {
#     density <- 0
#     for (i in 1:(length(qqTarg) - 1)) {
# 
#         if (x == qqTarg[i] && x == qqTarg[i+1]) {# Handle zero-length interval by using adjacent intervals. s
#       
#           if(i > 1 && i < length(qqTarg) - 1) #central values of the quantiles
#              density <- 1 / (qqTarg[i + 2] - qqTarg[i - 1]) * (QQ[i + 2] - QQ[i - 1])
#           else if (i == 1)
#              density <- 1 / (qqTarg[i + 2] - qqTarg[i]) * (QQ[i + 2] - QQ[i]) #Otherwise I'm out of index!
#           else if (i == length(qqTarg) - 1)
#             density <- 1 / (qqTarg[i + 1] - qqTarg[i - 1]) * (QQ[i + 1] - QQ[i - 1]) #Otherwise I'm out of index!
#           
#         return(density)
#       }
#       
#       else if (x >= qqTarg[i] && x < qqTarg[i + 1]) {
#         
#         if (abs(qqTarg[i] - qqTarg[i + 1]) < 0.02) { #with such a small interval, this create lots of problems giving high values in output
#           
#           if(i > 1 && i < length(qqTarg) - 1) #central values of the quantiles
#             density <- 1 / (qqTarg[i + 2] - qqTarg[i - 1]) * (QQ[i + 2] - QQ[i - 1])
#           else if (i == 1)
#             density <- 1 / (qqTarg[i + 2] - qqTarg[i]) * (QQ[i + 2] - QQ[i]) #Otherwise I'm out of index!
#           else if (i == length(qqTarg) - 1)
#             density <- 1 / (qqTarg[i + 1] - qqTarg[i - 1]) * (QQ[i + 1] - QQ[i - 1]) #Otherwise I'm out of index!
# 
#         }
#         
#         else density <- 1 / (qqTarg[i + 1] - qqTarg[i]) * (QQ[i + 1] - QQ[i])
#         
#         if (density > 1){
#           cat("Interval:", qqTarg[i], "-", qqTarg[i + 1], "Density:", density, "\n") #Just to check if I get insanely high values
#         }
#         return(density)
#       }
#       
#     }
#     # If x is outside the range of given QQ, return 0 as the density
#     return(0)
#   }
#   
#   # Apply find_density to each element in X
#   densities <- sapply(X, find_density)
#   
#   return(densities)
# }






dst <- function(X, QQ, qqTarg) {
  # Function to find density for a single point
  find_density <- function(x) {
    density <- 0
    for (i in 1:(length(qqTarg) - 1)) {
      
      # Function to dynamically find an interval that is not too small
      find_wider_interval <- function(index) {
        left_index <- index
        right_index <- index + 1
        while (abs(qqTarg[right_index] - qqTarg[left_index]) < 0.01) {
          if (left_index > 1) {
            left_index <- left_index - 1
          }
          if (right_index < length(qqTarg)) {
            right_index <- right_index + 1
          }
          
        }
        
        return(c(left_index, right_index))
      }
      
      if (x == qqTarg[i] && x == qqTarg[i + 1]) {
        indices <- find_wider_interval(i)
        density <- 1 / (qqTarg[indices[2]] - qqTarg[indices[1]]) * (QQ[indices[2]] - QQ[indices[1]])
        return(density)
      } 
      else if (x >= qqTarg[i] && x < qqTarg[i + 1]) {
        if (abs(qqTarg[i] - qqTarg[i + 1]) < 0.01) {
          indices <- find_wider_interval(i)
          density <- 1 / (qqTarg[indices[2]] - qqTarg[indices[1]]) * (QQ[indices[2]] - QQ[indices[1]])
        } 
        else {
          density <- 1 / (qqTarg[i + 1] - qqTarg[i]) * (QQ[i + 1] - QQ[i])
        }
        
        if (density > 1) {
          cat("Interval:", qqTarg[i], "-", qqTarg[i + 1], "Density:", density, "\n") # Just to check if I get insanely high values
        }
        return(density)
      }
      
    }
    # If x is outside the range of given QQ, return 0 as the density
    return(0)
  }
  
  # Apply find_density to each element in X
  densities <- sapply(X, find_density)
  
  return(densities)
}





qst <- function(QQ, qqTarg) { # qst is always called giving in input QQ. Without smoothing, the estimated quantiles are exactly qqTarg!
  if (length(QQ)== length(qqTarg)) {
    return(qqTarg)
  }
  else
    cat("wrong dimensions")
}





# Set forecast horizon (run script separately for h = 1 and h = 4)
h <- 1

loadsavedresults = FALSE; # If I ran code already results are stored in ResOOS_H11 and ResOOS_H14, in that case, set = TRUE


# Graphics settings - R's graphics system differs from MATLAB's, so some modifications are necessary
par(mfrow = c(1, 1))  # Reset plot window to single pane


# Load data 
file_path <- "DataVulnerabilityAppendix.xls"

# Read the file
data <- read_excel(file_path)
data <- data[,1:3]
hist(data$A191RL1Q225SBEA)
# Filter data for 1973Q1-2015Q4
colnames(data)[1] <- "Time"
data$Time <- as.Date(data$Time)


# Subset the data
data <- data[data$Time >= as.Date("1973-01-01") & data$Time <= as.Date("2015-10-01"), ]
X <- data[,2:3]
Time <- data$Time


# Set forecast settings
#QQ <- seq(0.05, 0.95, by = 0.05)
QQ <- seq(0.01, 0.99, by = 0.01) #Let's try a much more fine grid 
#QQ <- c(0.01, QQ, 0.99) # dst function returns too many zeros. This should help
deltaYY <- 0.1
YY <- seq(-20, 20, by = deltaYY)
jtFirstOOS <- which(year(data$Time) == 1993 & month(data$Time) == 1)
indices <- which(QQ %in% c(0.05, 0.25, 0.5, 0.75, 0.95))
jq05 <- indices[1]
jq25 <- indices[2]
jq50 <- indices[3]
jq75 <- 15 #couldn't automatically translate from MATLAB, I set it manually
jq95 <- indices[4] #couldn't automatically translate from MATLAB, I set it manually

# Construct average growth rates
y <- X$A191RL1Q225SBEA
Yh <- matrix(0, nrow=length(y), ncol=4)


Yh <- stats::filter(y, rep(1/h, h), sides=1) #If h = 1, y = Yh
if (h>1){
  Yh[1:(h-1)] <- NA
}
hist(Yh)


#Construct matrices of regressors
Z <- cbind(1, X[,2], y)
ZGDPonly <- cbind(1, y)
Z <-as.matrix(Z)


# Get length of Time and QQ/YY
len_time <- length(data$Time)
len_qq <- length(QQ)
len_yy <- length(YY)

# Initialize matrices to store forecasts
{
# Raw QQ
YQ_NaNs <- matrix(NA, len_time, len_qq)
#YQ_low_adj_ISC <- YQ_NaNs #IN the original script I've just YQ_ISC, because I do a point estimate of the quantile. Here I do a confidence interval estimation and I need 2
#YQ_high_adj_ISC <- YQ_NaNs
YQ_low_ISC <- YQ_NaNs
YQ_high_ISC <- YQ_NaNs
YQ_ISC <- YQ_NaNs
#YQ_low_adj_OOSC <- YQ_NaNs 
#YQ_high_adj_OOSC <- YQ_NaNs
YQ_low_OOSC <- YQ_NaNs
YQ_high_OOSC <- YQ_NaNs
YQ_OOSC <- YQ_NaNs
YQ_lowGDPonly_ISC <- YQ_NaNs
YQ_highGDPonly_ISC <- YQ_NaNs
YQGDPonly_ISC <- YQ_NaNs
YQGDPonly_OOSC <- YQ_NaNs
YQunc_ISC <- YQ_NaNs
YQunc_OOSC <- YQ_NaNs
YQunclow_ISC <- YQ_NaNs
YQunchigh_ISC <- YQ_NaNs
YQGDPonly_high_OOSC <- YQ_NaNs
YQGDPonly_low_OOSC <- YQ_NaNs
YQunclow_OOSC <- YQ_NaNs
YQunchigh_OOSC <- YQ_NaNs

YQunchigh_OOSC <- YQ_NaNs
YQunclow_OOSC <- YQ_NaNs


# PDFs (evaluated over grid)
P_NaNs <- matrix(NA, len_time, len_yy)
PST_ISC <- P_NaNs
PST_OOSC <- P_NaNs
PSTGDPonly_ISC <- P_NaNs
PSTGDPonly_OOSC <- P_NaNs
PSTunc_ISC <- P_NaNs
PSTunc_OOSC <- P_NaNs

# Smoothed QQ
Q_NaNs <- matrix(NA, len_time, len_qq)
QST_ISC <- Q_NaNs
QST_OOSC <- Q_NaNs
QSTGDPonly_ISC <- Q_NaNs
QSTGDPonly_OOSC <- Q_NaNs
QSTunc_ISC <- Q_NaNs
QSTunc_OOSC <- Q_NaNs

# CDFs (evaluated over grid)
C_NaNs <- matrix(NA, len_time, len_yy)
CST_ISC <- C_NaNs
CST_OOSC <- C_NaNs
CSTGDPonly_ISC <- C_NaNs
CSTGDPonly_OOSC <- C_NaNs
CSTunc_ISC <- C_NaNs
CSTunc_OOSC <- C_NaNs

# Skewed t-distribution parameters
STpar_NaNs <- matrix(NA, len_time, 4)
STpar_ISC <- STpar_NaNs
STpar_OOSC <- STpar_NaNs
STparGDPonly_ISC <- STpar_NaNs
STparGDPonly_OOSC <- STpar_NaNs
STparunc_ISC <- STpar_NaNs
STparunc_OOSC <- STpar_NaNs

# Predictive scores
Score_NaNs <- rep(NA, len_time)
ScoreST_ISC <- Score_NaNs
ScoreST_OOSC <- Score_NaNs
ScoreSTGDPonly_ISC <- Score_NaNs
ScoreSTGDPonly_OOSC <- Score_NaNs
ScoreSTunc_ISC <- Score_NaNs
ScoreSTunc_OOSC <- Score_NaNs

# Probability integral transforms
Pit_NaNs <- rep(NA, len_time)
PitST_ISC <- Pit_NaNs
PitST_OOSC <- Pit_NaNs
PitSTGDPonly_ISC <- Pit_NaNs
PitSTGDPonly_OOSC <- Pit_NaNs
PitSTunc_ISC <- Pit_NaNs
PitSTunc_OOSC <- Pit_NaNs

# Left entropy
Entropy_NaNs <- rep(NA, len_time)
LeftEntropy_ISC <- Entropy_NaNs
LeftEntropy_OOSC <- Entropy_NaNs

#Split creating I1 and I2
full_length <- length(Yh[(h + 1):length(Yh)])

#Let's do a 75-25 split

test_length <- full_length*50/100

#permuted_indices <- c(1,2,3,4,sample((h+1):length(Yh))) #if you want to permute
permuted_indices <- c(1:length(Yh)) # if you don't want to permute

Yh_perm <- Yh[permuted_indices]
Z_perm <- Z[permuted_indices,]
ZGDPonly_perm <- ZGDPonly[permuted_indices,]
Yh1 <- Yh_perm[(h+1):(h+test_length)]
Yh2 <- Yh_perm[(h+1+test_length):length(Yh)]
Z1 <- Z_perm[1:(test_length),]
Z2 <- Z_perm[(test_length+1):(length(Yh) - h),]
ZGDPonly1 <- ZGDPonly_perm[1:test_length,]
ZGDPonly2 <- ZGDPonly_perm[(test_length+1):(length(Yh) - h),]
}

if (loadsavedresults == FALSE) {
  
#-------------------    %% In-sample estimation of conditional QQ

for (jq in 1:length(QQ)) {
  
  #CQR is built for intervals, not QQ. Thus I create a prediction interval of the form [-Inf, quantile X] creating a prediction interval with
  #confidence X %
  
  
  #------ Conformalized Quantile regression with both NFCI and GDP
  
  YQ_low_ISC[(h + 1):(h + test_length), jq] <- -Inf 
  
  b_high <- rq(Yh1 ~ Z1[,-1], tau= QQ[jq])
  YQ_high_ISC[(h + 1):(h + full_length-test_length), jq] <- as.vector(Z2 %*% coef(b_high)) 

  
  # Initialize a vector for errors
  E_i <- rep(NA, length(Yh2))
  
  # Calculate errors for each point in the test set I2
  for (i in 1:length(E_i)) {
    E_i[i] <- max(YQ_low_ISC[h + i, jq] - Yh2[i], Yh2[i] - YQ_high_ISC[h + i, jq])
  }
  
  # Compute Q(QQ[jq])(E, I2) N.B 1 - ?? = QQ[jq]
  quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
  
  YQ_ISC[(h + 1):length(Yh), jq] <- as.vector(Z[1:(length(Yh) - h),] %*% coef(b_high)) + quantile_E
  
  
  
  #------ Conformalized Quantile regression with GDP only
  
  YQ_lowGDPonly_ISC[(h + 1):(h + test_length), jq] <- -Inf 
  
  b_highGDPonly <- rq(Yh1 ~ ZGDPonly1[,-1], tau= QQ[jq])
  YQ_highGDPonly_ISC[(h + 1):(h + full_length-test_length), jq] <- as.vector(ZGDPonly2 %*% coef(b_highGDPonly)) 

  # Initialize a vector for errors
  E_i <- rep(NA, length(Yh2))
  
  # Calculate errors for each point in the test set I2
  for (i in 1:length(E_i)) {
    E_i[i] <- max(YQ_lowGDPonly_ISC[h + i, jq] - Yh2[i], Yh2[i] - YQ_highGDPonly_ISC[h + i, jq])
  }
  
  # Compute Q(QQ[jq])(E, I2) N.B 1 - ?? = QQ[jq]
  quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
  
  YQGDPonly_ISC[(h + 1):length(Yh), jq] <- as.vector(ZGDPonly[1:(length(Yh) - h),] %*% coef(b_highGDPonly)) + quantile_E
  
  
  
  #------ Unconditional QQ (conformalized quantile regression on constant)
  
  YQunclow_ISC[(h + 1):(h + test_length), jq] <- - Inf 
  
  bunc_high <- rq(Yh1 ~ 1, tau=QQ[jq])
  YQunchigh_ISC[(h + 1):(h + test_length), jq] <- rep(coef(bunc_high), length((h + 1):(h + test_length)))

  # Initialize a vector for errors
  E_i <- rep(NA, length(Yh2))
  
  # Calculate errors for each point in the test set I2
  for (i in 1:length(E_i)) {
    E_i[i] <- max(YQunclow_ISC[h + i, jq] - Yh2[i], Yh2[i] - YQunchigh_ISC[h + i, jq])
  }
  
  # Compute Q(QQ[jq])(E, I2) N.B 1 - ?? = QQ[jq]
  quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
  
  YQunc_ISC[(h + 1):length(Yh), jq] <- rep(coef(bunc_high), length(Time) - h) + quantile_E
}


#---------    %% Fit skewed-t distribution for in-sample unconditional QQ

{
    # Fit skewed-t distribution for in-sample unconditional QQ
  qqTarg <- YQunc_ISC[nrow(YQunc_ISC), ]

  
  # Assign values to matrices based on the skewed-t fit
  densities <- dst(YY, QQ, qqTarg) # PROBLEMA! visto che ho solo quantili empirici, sotto quantile 0.05 e sopra 0.95 assegno densità =0. Cosa che con smoothing non succede. Come risolvere?
  replicated_matrix <- matrix(rep(densities, each = length(Time) - h), ncol = length(densities))
  PSTunc_ISC[(h + 1):nrow(PSTunc_ISC), ] <- replicated_matrix #Giusto, ma tanti 0, forse troppi che danno problemi
  
  
  densities <- qst(QQ, qqTarg) #diverso dall'originale che invece dava i quantili dopo smoothing
  replicated_matrix <- matrix(rep(densities, each = length(Time) - h), ncol = length(densities))
  QSTunc_ISC[(h + 1):nrow(QSTunc_ISC), ] <- replicated_matrix #giusto
  
  
  densities <- pst(YY, QQ, qqTarg)
  replicated_matrix <- matrix(rep(densities, each = length(Time) - h), ncol = length(densities))
  CSTunc_ISC[(h + 1):nrow(CSTunc_ISC), ] <- replicated_matrix #Credo sbagliato
  
  
  #STparunc_ISC[(h + 1):nrow(STparunc_ISC), ] <- matrix(rep(c(lc, sc, sh, df), times = length(Time) - h), 
  #                                                   nrow = length(Time) - h, 
  #                                                   byrow = TRUE)
  #Questo semplicemente salva i parametri, non mi serve più
  
  
  ScoreSTunc_ISC[(h + 1):length(Yh)] <- dst(Yh[(h + 1):length(Yh)], QQ, qqTarg) 
  PitSTunc_ISC[(h + 1):length(Yh)] <- pst(Yh[(h + 1):length(Yh)], QQ, qqTarg)
  
}

#---------------------    %% Out-of-sample estimation of conditional QQ



for (jt in 1:(length(Time) - h)) { 
  
  
    month_val <- as.numeric(format(Time[jt], "%m"))
    year_val <- as.numeric(format(Time[jt], "%Y"))
    
    if (month_val == 1 && jt >= jtFirstOOS) { #jtFirstOOS is the date since when I start computing Out-of-sample estimations
      cat(sprintf("Computing in-sample and out-of-sample predictive densities in %d", year_val), "\n")
    } else {
      cat(sprintf("Computing in-sample predictive densities in %d", year_val), "\n")
    }
    
    YhRealized <- Yh[jt + h]
    
    qqTarg <- YQ_ISC[jt + h, ]

    PST_ISC[jt + h, ] <- dst(YY, QQ, qqTarg) 
    QST_ISC[jt + h, ] <- qst(QQ, qqTarg)
    CST_ISC[jt + h, ] <- pst(YY, QQ, qqTarg)
    #STpar_ISC[jt + h, ] <- c(lc, sc, sh, df)
    ScoreST_ISC[jt + h ] <- dst(YhRealized, QQ, qqTarg)
    PitST_ISC[jt + h ] <- pst(YhRealized, QQ, qqTarg)    # is the probability to observe a value < of YhRealized in this distribution 
    
    Temp <- PST_ISC[jt + h, ] * (YY < QST_ISC[jt + h, jq50])
    
    non_zero_indexes <- (PSTunc_ISC[jt + h, ]!= 0) & (PST_ISC[jt + h, ] != 0)
    
    # Create new vectors with values from those indexes
    PSTunc_ISC_non_zero <- PSTunc_ISC[jt + h, ][non_zero_indexes]
    PST_ISC_non_zero <- PST_ISC[jt + h, ][non_zero_indexes]
    Temp_non_zero <- Temp[non_zero_indexes]
    
    LeftEntropy_ISC[jt + h] <- -sum((log(PSTunc_ISC_non_zero) - log(PST_ISC_non_zero)) * Temp_non_zero * deltaYY) 
    
    # Similar computations for GDP only
    qqTarg_GDPonly <- YQGDPonly_ISC[jt + h, ]
    # params <- QQInterpolation_env$QQInterpolation(qqTarg_GDPonly, QQ)
    # lc <- params$lc
    # sc <- params$sc
    # sh <- params$sh
    # df <- params$df
     
    PSTGDPonly_ISC[jt + h, ] <- dst(YY, QQ, qqTarg_GDPonly)
    QSTGDPonly_ISC[jt + h, ] <- qst(QQ, qqTarg_GDPonly)
    CSTGDPonly_ISC[jt + h, ] <- pst(YY, QQ, qqTarg_GDPonly)
    #STparGDPonly_ISC[jt + h, ] <- c(lc, sc, sh, df)
    ScoreSTGDPonly_ISC[jt + h] <- dst(YhRealized, QQ, qqTarg_GDPonly)
    PitSTGDPonly_ISC[jt + h] <- pst(YhRealized, QQ, qqTarg_GDPonly) # is the probability to observe a value < of YhRealized in this distribution 
     
    
    
    if (jt >= jtFirstOOS) {
      if (month(Time[jt]) == 1) {
        cat(sprintf("Now computing the real-time predictive densities in %d", year(Time[jt])), "\n")
      }
      
      for (jq in 1:length(QQ)) {
        #------- Conformalized Quantile Regression with both NFCI and GDP, out-of-sample
        
        
        #Split creating I1 and I2
        full_length <- length(Yh[(h + 1):jt])
        test_length = full_length*50/100
        Yh1 <- Yh[(h+1):(h+test_length)]
        Yh2 <- Yh[(h+1+test_length):jt] #Yh1 and Yh2 correctly have same dimension
        Z1 <- Z[1:test_length,]
        Z2 <- Z[(test_length+1):(jt - h),] #Z1 and Z2 correctly have same dimension
        ZGDPonly1 <- ZGDPonly[1:test_length,]
        ZGDPonly2 <- ZGDPonly[(test_length+1):(jt - h),]
        
        
        #b_low <- rq(Yh1 ~ Z1[,-1], 0.01)
        YQ_low_OOSC[(h + 1):(h + test_length), jq] <- -Inf #as.vector(Z2 %*% coef(b_low)) #at each iteration of jt, it will be overwritten. But it's fine, we just need to extract jt + h row later on
        
        
        b_high <- rq(Yh1 ~ Z1[,-1], tau= QQ[jq]) #Train on I1
        YQ_high_OOSC[(h + 1):(h + full_length-test_length), jq] <- as.vector(Z2 %*% coef(b_high)) #Evaluate on I2

        # Initialize a vector for errors
        E_i <- rep(NA, length(Yh2))
        
        # Calculate errors for each point in the test set I2
        for (i in 1:length(E_i)) {
          E_i[i] <- max(YQ_low_OOSC[h + i, jq] - Yh2[i], Yh2[i] - YQ_high_OOSC[h + i, jq])
        }
        
        # Compute Q(1-alpha)(E, I2)
        quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
        
        # YQ_low_adj_OOSC[jt + h, jq] <- Z[jt,] %*% coef(b_low) - quantile_E 
        YQ_OOSC[jt + h, jq] <- Z[jt,] %*% coef(b_high) + quantile_E 
        

        
        #------- Quantile regression with GDP only, out-of-sample
        
        YQGDPonly_low_OOSC[(h + 1):(h + test_length), jq] <- -Inf
        
        
        bGDPonly_high <- rq(Yh1 ~ ZGDPonly1[,-1], tau= QQ[jq]) #Train on I1
        YQGDPonly_high_OOSC[(h + 1):(h + full_length-test_length), jq] <- as.vector(ZGDPonly2 %*% coef(bGDPonly_high)) #Evaluate on I2
        # Initialize a vector for errors
        E_i <- rep(NA, length(Yh2))
        
        # Calculate errors for each point in the test set I2
        for (i in 1:length(E_i)) {
          E_i[i] <- max(YQGDPonly_low_OOSC[h + i, jq] - Yh2[i], Yh2[i] - YQGDPonly_high_OOSC[h + i, jq])
        }
        
        # Compute Q(1-alpha)(E, I2)
        quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
        
        # YQ_low_adj_OOSC[jt + h, jq] <- Z[jt,] %*% coef(b_low) - quantile_E 
        YQGDPonly_OOSC[jt + h, jq] <- ZGDPonly[jt,] %*% coef(bGDPonly_high) + quantile_E 
        

        
        
        #------- Quantile regression with Unconditional QQ, out-of-sample
        
        
        YQunclow_OOSC[(h + 1):(h + test_length), jq] <- - Inf #rep(coef(bunc_low), length((h + 1):(h + full_length/2)))
        
        bunc_high <- rq(Yh1 ~ 1, tau=QQ[jq])
        YQunchigh_OOSC[(h + 1):(h + test_length), jq] <- rep(coef(bunc_high), length((h + 1):(h + test_length)))

        # Initialize a vector for errors
        E_i <- rep(NA, length(Yh2))
        
        # Calculate errors for each point in the test set I2
        for (i in 1:length(E_i)) {
          E_i[i] <- max(YQunclow_OOSC[h + i, jq] - Yh2[i], Yh2[i] - YQunchigh_OOSC[h + i, jq])
        }
        
        # Compute Q(1-??)(E, I2)
        quantile_E <- quantile(E_i, pmin(1, pmax(0, (QQ[jq]) * (1 + 1/length(Yh2)))))
        
        YQunc_OOSC[jt + h, jq] <- coef(bunc_high) + quantile_E
        
      }
      
      #params <- QQInterpolation_env$QQInterpolation(YQ_OOSC[jt + h, ], QQ) #YQ_OOSC[jt + h, ] is the new qqTarg!
      PST_OOSC[jt + h, ] <- dst(YY, QQ, YQ_OOSC[jt + h, ])
      QST_OOSC[jt + h, ] <- qst(QQ, YQ_OOSC[jt + h, ])
      CST_OOSC[jt + h, ] <- pst(YY, QQ, YQ_OOSC[jt + h, ])
      #STpar_OOSC[jt + h, ] <- c(params$lc, params$sc, params$sh, params$df)
      ScoreST_OOSC[jt + h] <- dst(YhRealized, QQ, YQ_OOSC[jt + h, ])
      PitST_OOSC[jt + h] <- pst(YhRealized, QQ, YQ_OOSC[jt + h, ]) # is the probability to observe a value < of YhRealized in this distribution 
      
      #params_GDPonly <- QQInterpolation_env$QQInterpolation(YQGDPonly_OOSC[jt + h, ], QQ)
      PSTGDPonly_OOSC[jt + h, ] <- dst(YY, QQ, YQGDPonly_OOSC[jt + h, ])
      QSTGDPonly_OOSC[jt + h, ] <- qst(QQ, YQGDPonly_OOSC[jt + h, ])
      CSTGDPonly_OOSC[jt + h, ] <- pst(YY, QQ, YQGDPonly_OOSC[jt + h, ])
      #STparGDPonly_OOSC[jt + h, ] <- c(params_GDPonly$lc, params_GDPonly$sc, params_GDPonly$sh, params_GDPonly$df)
      ScoreSTGDPonly_OOSC[jt + h] <- dst(YhRealized, QQ, YQGDPonly_OOSC[jt + h, ])
      PitSTGDPonly_OOSC[jt + h] <- pst(YhRealized, QQ, YQGDPonly_OOSC[jt + h, ]) # is the probability to observe a value < of YhRealized in this distribution 
       
      #params_unc <- QQInterpolation_env$QQInterpolation(YQunc_OOSC[jt + h, ], QQ)
      PSTunc_OOSC[jt + h, ] <- dst(YY, QQ, YQunc_OOSC[jt + h, ])
      QSTunc_OOSC[jt + h, ] <- qst(QQ, YQunc_OOSC[jt + h, ])
      CSTunc_OOSC[jt + h, ] <- pst(YY, QQ, YQunc_OOSC[jt + h, ])
      #STparunc_OOSC[jt + h, ] <- c(params_unc$lc, params_unc$sc, params_unc$sh, params_unc$df)
      ScoreSTunc_OOSC[jt + h] <- dst(YhRealized, QQ, YQunc_OOSC[jt + h, ])
      PitSTunc_OOSC[jt + h] <- pst(YhRealized, QQ, YQunc_OOSC[jt + h, ]) # is the probability to observe a value < of YhRealized in this distribution 
       
      # Compute entropy for skewed t-distribution from quantile regression with GDP and NFCI, out-of-sample
      Temp <- PST_OOSC[jt + h, ] * (YY < QST_OOSC[jt + h, jq50])

      non_zero_indexes <- (PSTunc_OOSC[jt + h, ] != 0) & (PST_OOSC[jt + h, ] != 0)
      
      # Create new vectors with values from those indexes
      PSTunc_OOSC_non_zero <- PSTunc_OOSC[jt + h, ][non_zero_indexes]
      PST_OOSC_non_zero <- PST_OOSC[jt + h, ][non_zero_indexes]
      Temp_non_zero <- Temp[non_zero_indexes]
      
      LeftEntropy_OOSC[jt + h] <- -sum((log(PSTunc_OOSC_non_zero) - log(PST_OOSC_non_zero)) * Temp_non_zero * deltaYY)
    }
    
  } 
  

# # 
# # 
#  filename <- paste("ResOOSCnew_H", h, ".RData",sep="")
#   cat(paste("Saving results to file", filename, "\n"))
#   
#   # Save all the variables to the .RData file
#   save(
#     YQ_ISC,      YQ_OOSC,      YQGDPonly_ISC,      YQGDPonly_OOSC,      YQunc_ISC,      YQunc_OOSC,
#     PST_ISC,     PST_OOSC,     PSTGDPonly_ISC,     PSTGDPonly_OOSC,     PSTunc_ISC,     PSTunc_OOSC,
#     QST_ISC,     QST_OOSC,     QSTGDPonly_ISC,     QSTGDPonly_OOSC,     QSTunc_ISC,     QSTunc_OOSC,
#     CST_ISC,     CST_OOSC,     CSTGDPonly_ISC,     CSTGDPonly_OOSC,     CSTunc_ISC,     CSTunc_OOSC,
#     STpar_ISC,   STpar_OOSC,   STparGDPonly_ISC,   STparGDPonly_OOSC,   STparunc_ISC,   STparunc_OOSC,
#     ScoreST_ISC, ScoreST_OOSC, ScoreSTGDPonly_ISC, ScoreSTGDPonly_OOSC, ScoreSTunc_ISC, ScoreSTunc_OOSC,
#     PitST_ISC,   PitST_OOSC,   PitSTGDPonly_ISC,   PitSTGDPonly_OOSC,   PitSTunc_ISC,   PitSTunc_OOSC,
#     LeftEntropy_ISC, LeftEntropy_OOSC, 
#     file=filename
#   )
#   
  }
# 
# 
# #-------------------------------- per caricare dati salvati -----------------------
# 
# filename <- paste("ResOOS_H", h, "_50-50.RData", sep="")
# cat(paste("Loading results from file", filename, "\n"))
# 
# load(filename)
# 
# #-----------------------------------------------------------------------------------



PITtest_env <- new.env()
source("PITtest.r",local = PITtest_env)

rstestboot_env <- new.env()
source("rstestboot.r",local = rstestboot_env)







# Figure 10. Out-of-sample Predictions.             

# (a)/(b) QQ
par(mar = c(3, 3, 2, 1))  # Adjust the values as needed (bottom, left, top, right)

plot(Time, YQ_OOSC[, jq05], type = 'l', col = 'blue', xlab = 'Time', ylab = 'QQ', xlim = range(Time),ylim = c(-20,20))
lines(Time, YQ_OOSC[, jq50], type = 'l', col = 'blue', lty = 2)
lines(Time, YQ_OOSC[,jq95], type = 'l', col = 'blue', lty = 3) 
lines(Time, YQ_ISC[, jq05], type = 'l', col = 'black', lty = 1)
lines(Time, YQ_ISC[, jq50], type = 'l', col = 'black', lty = 2)
lines(Time, YQ_ISC[, jq95], type = 'l', col = 'black', lty = 3)





# (c)/(d) Downside Entropy        
plot(Time, LeftEntropy_OOSC, type = 'l', col = 'blue', 
     xlab = 'Time', ylab = 'Entropy', xlim = range(Time))
lines(Time, LeftEntropy_ISC, type = 'l', col = 'black', lty = 2)



# Figure 11. Out-of-sample Accuracy.   
# (a)/(b) Predictive scores
plot(Time, ScoreST_OOSC, type = 'l', col = 'blue', xlab = 'Time', ylab = 'Scores')
lines(Time, ScoreSTGDPonly_OOSC, type = 'l', col = 'black', lty = 2)
legend('topleft', legend = c('GDP and NFCI', 'GDP only'))

# h = 4:> mean(ScoreST_OOSC - ScoreST_OOSCpaper, na.rm = TRUE) == 0.004654713. Predictive scores of my model are better. 
# h = 4:> sum(ScoreST_OOSC - ScoreST_OOSCpaper, na.rm = TRUE) == 0.4096147. Confirmed by using sum instead of mean
# h = 4:> mean(ScoreSTGDPonly_OOSC - ScoreSTGDPonly_OOSC1, na.rm = TRUE) == 0.04962592. Also the model with only GDP performs better
# h = 4:> sum(ScoreSTGDPonly_OOSC - ScoreSTGDPonly_OOSC1, na.rm = TRUE) == 4.367081. Confirmed by using sum instead of mean

# h = 1:> mean(ScoreST_OOSC - ScoreST_OOSC1, na.rm = TRUE) == 0.01211243 Predictive scores of my model are better. 
# h = 1:> sum(ScoreST_OOSC - ScoreST_OOSC1, na.rm = TRUE) == 1.102231 Confirmed by using sum instead of mean
# h = 1:> mean(ScoreSTGDPonly_OOSC - ScoreSTGDPonly_OOSC1, na.rm = TRUE) == 0.02495498 Also the model with only GDP performs better
# h = 1:> sum(ScoreSTGDPonly_OOSC - ScoreSTGDPonly_OOSC1, na.rm = TRUE) #== 2.270903 Confirmed by using sum instead of mean
plot(ecdf(ScoreSTGDPonly_OOSC), main="PS ECDF Comparison GDPonly", col="blue", lty=1, lwd=2)
lines(ecdf(ScoreSTGDPonly_OOSC1), col="red", lty=2, lwd=2)
legend("bottomright", legend=c("Mio", "Paper"), col=c("blue", "red"), lty=c(1, 2), lwd=2)





# (c)/(d): PITs
# The code below was modified from files provided by Barbara Rossi and
# Tatevik Sekhposyan implementing the specification tests for predictive
# densities described in Rossi and Sekhposyan (2017).
rvec <- seq(0, 1, by = 0.001)
zST_ecdf <- PITtest_env$PITtest(PitST_OOSC, rvec)
CnSS_model <- 1 - mean((zST_ecdf - seq(0, 1, length.out = length(zST_ecdf)))^2) / var(zST_ecdf) # h = 4: 0.67701012121 || h = 1: 0.9366  is the value. 1 is perfect calibrated
mean_squared_diff <- mean((zST_ecdf-rvec)^2) # h = 4: 0.02338473 || h = 1: 0.005109203

zSTGDPonly_ecdf <- PITtest_env$PITtest(PitSTGDPonly_OOSC, rvec)
CnSS_model_GDPonly <- 1 - mean((zSTGDPonly_ecdf - seq(0, 1, length.out = length(zSTGDPonly_ecdf)))^2) / var(zSTGDPonly_ecdf) # h = 4: 0.872747225 || h = 1: 0.95697 is the value. 1 is perfect calibrated
mean_squared_diff_GDPonly <- mean((zSTGDPonly_ecdf-rvec)^2) # h = 4: 0.009966178 || h = 1: 0.003439986

if (h == 1) {
  # Use asymptotic 5% critical value from Rossi and Sekhposyan (2017): 1.34
  kappa <- 1.34
  kappaGDPonly <- 1.34
  
} else if (h == 4) {
  # Compute bootstrapped 5% critical values
  PITs <- cbind(PitST_OOSC, PitSTGDPonly_OOSC)
  PITs <- PITs[(jtFirstOOS + h):nrow(PITs), , drop = FALSE]
  
  #testcritvalues <- matrix(NA, nrow = 1, ncol = 2, dimnames = list(NULL, c('GDP and NFCI', 'GDP only')))
  testcritvalues <- array(NA, dim = c(2, 3, 2))
  
  for (i in 1:2) {
    testcritvalues[,, i] <- round(rstestboot_env$rstestboot(PITs[, i])$critvalues[2] * 100) / 100
  }
  
  kappa <- testcritvalues[1, 2, 1] #different from Matlab due to seed in CVfinalbootstrapInoue
  kappaGDPonly <- testcritvalues[1, 2, 2] #different from Matlab due to seed in CVfinalbootstrapInoue
}

# Plot PIT for full quantile regression vs. quantile regression with GDP only
plot(rvec, zST_ecdf, type = 'l', col = 'blue', xlab = '??', ylab = 'Empirical CDF')
lines(rvec, zSTGDPonly_ecdf, type = 'l', col = 'red')
P <- sum(!is.na(PitST_OOSC)) #correct for both h = 1 and h = 4

lines(rvec, rvec - (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec, rvec + (kappa / sqrt(P)), col = 'black',lty=2)
lines(rvec,rvec , col = 'black',lty=2)

legend('bottomright', legend = c('GDP and NFCI', 'GDP only', 'Theoretical and 5% Critical Values'), cex = 0.5,fill = c('blue', 'red', 'black'))

#For hlibrary(



library(ggplot2)
library(dplyr)
library(tidyr)

# Assuming rvec, zST_ecdf, zSTGDPonly_ecdf are vectors of the same length
rvec <- seq(0, 1, by = 0.001)
zST_ecdf <- PITtest_env$PITtest(PitST_OOSC, rvec)
zSTGDPonly_ecdf <- PITtest_env$PITtest(PitSTGDPonly_OOSC, rvec)

# Check the contents of zST_ecdf and zSTGDPonly_ecdf
print(head(zST_ecdf))
print(head(zSTGDPonly_ecdf))

data1 <- data.frame(
  tau = rvec,
  full = zST_ecdf,
  GDPonly = zSTGDPonly_ecdf
)

# Create the Line data separately
line_data <- data.frame(tau = rvec, Line = rvec)


# Reshape data for ggplot
data1_long <- data1 %>%
  pivot_longer(cols = -tau, names_to = "Series", values_to = "Value")


# Define a common theme
custom_theme <- theme_minimal() +
  theme(
    axis.title = element_text(face = "bold", size = 14),
    axis.text = element_text(size = 12),
    legend.position = "top",
    plot.background = element_rect(fill = "white", color = NA),
    panel.background = element_rect(fill = "white", color = NA),
    panel.grid.major = element_line(color = "grey80"),
    panel.grid.minor = element_line(color = "grey90")
  )

# Calculate kappa and P
if (h == 1) {
  kappa <- 1.34
} else if (h == 4) {
  PITs <- cbind(PitST_OOSC, PitSTGDPonly_OOSC)
  PITs <- PITs[(jtFirstOOS + h):nrow(PITs), , drop = FALSE]
  
  testcritvalues <- array(NA, dim = c(2, 3, 2))
  
  for (i in 1:2) {
    testcritvalues[,, i] <- round(rstestboot_env$rstestboot(PITs[, i])$critvalues[2] * 100) / 100
  }
  
  kappa <- testcritvalues[1, 2, 1]
}

P <- sum(!is.na(PitST_OOSC))

# Create the data frame for the dashed lines
dashed_lines <- data.frame(
  tau = rvec,
  lower = rvec - (kappa / sqrt(P)),
  upper = rvec + (kappa / sqrt(P))
)

# Plot
plot1 <- ggplot(data1_long, aes(x = tau, y = Value, color = Series, linetype = Series)) +
  geom_line(size = 1) +
  geom_line(data = line_data, aes(x = tau, y = Line), color = "black", linetype = "dashed", size = 1.2, show.legend = FALSE) +
  geom_line(data = dashed_lines, aes(x = tau, y = lower), color = "black", linetype = "dashed", size = 1, show.legend = FALSE) +
  geom_line(data = dashed_lines, aes(x = tau, y = upper), color = "black", linetype = "dashed", size = 1, show.legend = FALSE) +
  scale_color_manual(values = c("full" = "blue", "GDPonly" = "red")) +
  scale_linetype_manual(values = c("full" = "solid", "GDPonly" = "solid")) +
  labs(x = expression(tau), y = 'Empirical CDF') +
  custom_theme +
  guides(color = guide_legend(title = NULL), linetype = guide_legend(title = NULL))

print(plot1)

